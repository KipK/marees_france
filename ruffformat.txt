--- custom_components/marees_france/__init__.py
+++ custom_components/marees_france/__init__.py
@@ -4,6 +4,7 @@
 This component provides tide, coefficient, and water level information for French harbors.
 It sets up sensors, services, and handles data fetching and caching.
 """
+
 from __future__ import annotations
 
 import logging
@@ -67,6 +68,7 @@
 
 CONFIG_SCHEMA = cv.config_entry_only_config_schema("marees_france")
 
+
 class CannotConnect(HomeAssistantError):
     """Error to indicate a failure to connect to the SHOM API."""
 
@@ -106,14 +108,16 @@
                 harbor_name = properties["toponyme"]
                 harbors[harbor_id] = {
                     "display": f"{harbor_name} ({harbor_id})",
-                    "name": harbor_name
+                    "name": harbor_name,
                 }
 
         if not harbors:
             _LOGGER.error("No harbors found in the response.")
             raise CannotConnect("No harbors found")
 
-        result_harbors = dict(sorted(harbors.items(), key=lambda item: item[1]["display"]))
+        result_harbors = dict(
+            sorted(harbors.items(), key=lambda item: item[1]["display"])
+        )
 
     except asyncio.TimeoutError as err:
         _LOGGER.error("Timeout fetching harbor list: %s", err)
@@ -143,7 +147,8 @@
     """
     _LOGGER.debug(
         "Migrating config entry %s from version %s",
-        config_entry.entry_id, config_entry.version
+        config_entry.entry_id,
+        config_entry.version,
     )
 
     if config_entry.version == 1:
@@ -152,7 +157,8 @@
 
         if not harbor_id:
             _LOGGER.error(
-                "Cannot migrate config entry %s: Missing harbor_id", config_entry.entry_id
+                "Cannot migrate config entry %s: Missing harbor_id",
+                config_entry.entry_id,
             )
             return False
 
@@ -162,13 +168,14 @@
         except CannotConnect as err:
             _LOGGER.error(
                 "Migration failed for entry %s: Could not fetch harbor list: %s",
-                config_entry.entry_id, err
+                config_entry.entry_id,
+                err,
             )
             return False
         except Exception:
             _LOGGER.exception(
                 "Migration failed for entry %s: Unexpected error fetching harbor list",
-                config_entry.entry_id
+                config_entry.entry_id,
             )
             return False
 
@@ -178,20 +185,18 @@
                 "Migration failed for entry %s: Harbor ID '%s' not found "
                 "in fetched list or missing 'name'",
                 config_entry.entry_id,
-                harbor_id
+                harbor_id,
             )
             return False
 
         harbor_name = harbor_details["name"]
         new_data[CONF_HARBOR_NAME] = harbor_name
 
-        hass.config_entries.async_update_entry(
-            config_entry, data=new_data, version=2
-        )
+        hass.config_entries.async_update_entry(config_entry, data=new_data, version=2)
         _LOGGER.info(
             "Successfully migrated config entry %s to version 2, added harbor_name: %s",
             config_entry.entry_id,
-            harbor_name
+            harbor_name,
         )
 
     elif config_entry.version > 2:
@@ -199,29 +204,38 @@
             "Cannot migrate config entry %s: Config entry version %s is newer than "
             "integration version 2",
             config_entry.entry_id,
-            config_entry.version
+            config_entry.version,
         )
         return False
 
     _LOGGER.debug("Migration check complete for config entry %s", config_entry.entry_id)
     return True
 
+
 # Service Schemas
-SERVICE_GET_WATER_LEVELS_SCHEMA = vol.Schema({
-    vol.Required("device_id"): cv.string,
-    vol.Required(ATTR_DATE): vol.Match(r"^\d{4}-\d{2}-\d{2}$"),
-})
-SERVICE_GET_TIDES_DATA_SCHEMA = vol.Schema({
-    vol.Required("device_id"): cv.string,
-})
-SERVICE_GET_COEFFICIENTS_DATA_SCHEMA = vol.Schema({
-    vol.Required("device_id"): cv.string,
-    vol.Optional(ATTR_DATE): vol.Match(r"^\d{4}-\d{2}-\d{2}$"),
-    vol.Optional("days"): cv.positive_int,
-})
-SERVICE_REINITIALIZE_HARBOR_DATA_SCHEMA = vol.Schema({
-    vol.Required("device_id"): cv.string,
-})
+SERVICE_GET_WATER_LEVELS_SCHEMA = vol.Schema(
+    {
+        vol.Required("device_id"): cv.string,
+        vol.Required(ATTR_DATE): vol.Match(r"^\d{4}-\d{2}-\d{2}$"),
+    }
+)
+SERVICE_GET_TIDES_DATA_SCHEMA = vol.Schema(
+    {
+        vol.Required("device_id"): cv.string,
+    }
+)
+SERVICE_GET_COEFFICIENTS_DATA_SCHEMA = vol.Schema(
+    {
+        vol.Required("device_id"): cv.string,
+        vol.Optional(ATTR_DATE): vol.Match(r"^\d{4}-\d{2}-\d{2}$"),
+        vol.Optional("days"): cv.positive_int,
+    }
+)
+SERVICE_REINITIALIZE_HARBOR_DATA_SCHEMA = vol.Schema(
+    {
+        vol.Required("device_id"): cv.string,
+    }
+)
 
 
 async def async_handle_reinitialize_harbor_data(call: ServiceCall) -> None:
@@ -246,15 +260,18 @@
     if not device_entry or not device_entry.config_entries:
         _LOGGER.error(
             "Reinitialize Service: Device %s not found or not linked to Marées France.",
-            device_id
+            device_id,
         )
-        raise HomeAssistantError(f"Device {device_id} not found or not linked to Marées France.")
+        raise HomeAssistantError(
+            f"Device {device_id} not found or not linked to Marées France."
+        )
     config_entry_id = next(iter(device_entry.config_entries))
     config_entry = hass.config_entries.async_get_entry(config_entry_id)
     if not config_entry or config_entry.domain != DOMAIN:
         _LOGGER.error(
             "Reinitialize Service: Config entry %s not found or not for %s.",
-            config_entry_id, DOMAIN
+            config_entry_id,
+            DOMAIN,
         )
         raise HomeAssistantError(
             f"Config entry {config_entry_id} not found or not for {DOMAIN}"
@@ -263,11 +280,16 @@
 
     _LOGGER.info(
         "Reinitialize Service: Starting data reinitialization for device %s (harbor: %s)",
-        device_id, harbor_id
+        device_id,
+        harbor_id,
     )
 
-    tides_store = Store[dict[str, dict[str, Any]]](hass, TIDES_STORAGE_VERSION, TIDES_STORAGE_KEY)
-    coeff_store = Store[dict[str, dict[str, Any]]](hass, COEFF_STORAGE_VERSION, COEFF_STORAGE_KEY)
+    tides_store = Store[dict[str, dict[str, Any]]](
+        hass, TIDES_STORAGE_VERSION, TIDES_STORAGE_KEY
+    )
+    coeff_store = Store[dict[str, dict[str, Any]]](
+        hass, COEFF_STORAGE_VERSION, COEFF_STORAGE_KEY
+    )
     water_level_store = Store[dict[str, dict[str, Any]]](
         hass, WATERLEVELS_STORAGE_VERSION, WATERLEVELS_STORAGE_KEY
     )
@@ -286,28 +308,40 @@
             del coeff_cache_full[harbor_id]
             await coeff_store.async_save(coeff_cache_full)
             caches_cleared.append("coefficients")
-            _LOGGER.debug("Reinitialize Service: Cleared coefficients cache for %s", harbor_id)
+            _LOGGER.debug(
+                "Reinitialize Service: Cleared coefficients cache for %s", harbor_id
+            )
 
         water_level_cache_full = await water_level_store.async_load() or {}
         if harbor_id in water_level_cache_full:
             del water_level_cache_full[harbor_id]
             await water_level_store.async_save(water_level_cache_full)
             caches_cleared.append("water levels")
-            _LOGGER.debug("Reinitialize Service: Cleared water levels cache for %s", harbor_id)
+            _LOGGER.debug(
+                "Reinitialize Service: Cleared water levels cache for %s", harbor_id
+            )
 
         if caches_cleared:
             _LOGGER.info(
                 "Reinitialize Service: Successfully cleared cache(s) for %s: %s",
-                harbor_id, ", ".join(caches_cleared)
+                harbor_id,
+                ", ".join(caches_cleared),
             )
         else:
-            _LOGGER.info("Reinitialize Service: No cache entries found to clear for %s", harbor_id)
+            _LOGGER.info(
+                "Reinitialize Service: No cache entries found to clear for %s",
+                harbor_id,
+            )
 
     except Exception as e:
-        _LOGGER.exception("Reinitialize Service: Error clearing cache for %s", harbor_id)
+        _LOGGER.exception(
+            "Reinitialize Service: Error clearing cache for %s", harbor_id
+        )
         raise HomeAssistantError(f"Error clearing cache for {harbor_id}: {e}") from e
 
-    _LOGGER.info("Reinitialize Service: Triggering immediate data refetch for %s", harbor_id)
+    _LOGGER.info(
+        "Reinitialize Service: Triggering immediate data refetch for %s", harbor_id
+    )
     fetch_errors = []
     try:
         tides_cache_full = await tides_store.async_load() or {}
@@ -319,15 +353,24 @@
         yesterday_str = yesterday.strftime(DATE_FORMAT)
         fetch_duration = 8
         if not await _async_fetch_and_store_tides(
-            hass, tides_store, tides_cache_full, harbor_id, yesterday_str, fetch_duration
+            hass,
+            tides_store,
+            tides_cache_full,
+            harbor_id,
+            yesterday_str,
+            fetch_duration,
         ):
             fetch_errors.append("tides")
 
         first_day_of_current_month = today.replace(day=1)
         coeff_fetch_days = 365
         if not await _async_fetch_and_store_coefficients(
-            hass, coeff_store, coeff_cache_full, harbor_id,
-            first_day_of_current_month, coeff_fetch_days
+            hass,
+            coeff_store,
+            coeff_cache_full,
+            harbor_id,
+            first_day_of_current_month,
+            coeff_fetch_days,
         ):
             fetch_errors.append("coefficients")
 
@@ -339,7 +382,8 @@
 
     except Exception as e:
         _LOGGER.exception(
-            "Reinitialize Service: Unexpected error during data refetch for %s", harbor_id
+            "Reinitialize Service: Unexpected error during data refetch for %s",
+            harbor_id,
         )
         raise HomeAssistantError(
             f"Unexpected error during data refetch for {harbor_id}: {e}"
@@ -348,7 +392,8 @@
     if fetch_errors:
         _LOGGER.error(
             "Reinitialize Service: Failed to refetch the following data for %s: %s",
-            harbor_id, ", ".join(fetch_errors)
+            harbor_id,
+            ", ".join(fetch_errors),
         )
         raise HomeAssistantError(
             f"Failed to refetch data for {harbor_id}: {', '.join(fetch_errors)}"
@@ -356,29 +401,28 @@
 
     _LOGGER.info(
         "Reinitialize Service: Successfully completed data reinitialization and refetch for %s",
-        harbor_id
+        harbor_id,
     )
 
-    coordinator: MareesFranceUpdateCoordinator | None = (
-        hass.data.get(DOMAIN, {}).get(config_entry.entry_id)
+    coordinator: MareesFranceUpdateCoordinator | None = hass.data.get(DOMAIN, {}).get(
+        config_entry.entry_id
     )
     if coordinator:
         _LOGGER.info(
             "Reinitialize Service: Requesting immediate coordinator update for %s",
-            harbor_id
+            harbor_id,
         )
         await coordinator.async_request_refresh()
     else:
         _LOGGER.warning(
             "Reinitialize Service: Could not find coordinator instance for %s "
-            "to trigger refresh.", harbor_id
+            "to trigger refresh.",
+            harbor_id,
         )
 
 
 async def async_check_and_prefetch_water_levels(
-    hass: HomeAssistant,
-    entry: ConfigEntry,
-    store: Store[dict[str, dict[str, Any]]]
+    hass: HomeAssistant, entry: ConfigEntry, store: Store[dict[str, dict[str, Any]]]
 ) -> None:
     """Check cache for the next 8 days and prefetch missing water level data.
 
@@ -388,14 +432,12 @@
         store: The store object for water level data.
     """
     harbor_id = entry.data[CONF_HARBOR_ID]
-    _LOGGER.info(
-        "Starting water level prefetch check for harbor: %s", harbor_id
-    )
+    _LOGGER.info("Starting water level prefetch check for harbor: %s", harbor_id)
     cache = await store.async_load() or {}
     today = date.today()
     missing_dates = []
 
-    for i in range(8): # Check today + next 7 days
+    for i in range(8):  # Check today + next 7 days
         check_date = today + timedelta(days=i)
         check_date_str = check_date.strftime("%Y-%m-%d")
         if check_date_str not in cache.get(harbor_id, {}):
@@ -415,7 +457,9 @@
     )
 
     for i, date_str in enumerate(missing_dates):
-        await _async_fetch_and_store_water_level(hass, store, cache, harbor_id, date_str)
+        await _async_fetch_and_store_water_level(
+            hass, store, cache, harbor_id, date_str
+        )
         if i < len(missing_dates) - 1:
             await asyncio.sleep(2)
 
@@ -451,16 +495,22 @@
     if not device_entry:
         raise HomeAssistantError(f"Device not found: {device_id}")
     if not device_entry.config_entries:
-        raise HomeAssistantError(f"Device {device_id} not associated with a config entry")
+        raise HomeAssistantError(
+            f"Device {device_id} not associated with a config entry"
+        )
     config_entry_id = next(iter(device_entry.config_entries))
     config_entry = hass.config_entries.async_get_entry(config_entry_id)
     if not config_entry or config_entry.domain != DOMAIN:
-        raise HomeAssistantError(f"Config entry {config_entry_id} not found or not for {DOMAIN}")
+        raise HomeAssistantError(
+            f"Config entry {config_entry_id} not found or not for {DOMAIN}"
+        )
     harbor_id = config_entry.data[CONF_HARBOR_ID]
 
     _LOGGER.debug(
         "Service call get_water_levels for device %s (harbor: %s), date: %s",
-        device_id, harbor_id, date_str
+        device_id,
+        harbor_id,
+        date_str,
     )
 
     store = Store[dict[str, dict[str, Any]]](
@@ -531,9 +581,7 @@
 
 
 async def async_check_and_prefetch_tides(
-    hass: HomeAssistant,
-    entry: ConfigEntry,
-    store: Store[dict[str, dict[str, Any]]]
+    hass: HomeAssistant, entry: ConfigEntry, store: Store[dict[str, dict[str, Any]]]
 ) -> None:
     """Check tide cache (yesterday to today+7 days) and prefetch if needed.
 
@@ -545,23 +593,27 @@
         store: The store object for tide data.
     """
     harbor_id = entry.data[CONF_HARBOR_ID]
-    _LOGGER.info("Marées France: Starting tide data prefetch check for harbor: %s", harbor_id)
+    _LOGGER.info(
+        "Marées France: Starting tide data prefetch check for harbor: %s", harbor_id
+    )
     cache = await store.async_load() or {}
     today = date.today()
     yesterday = today - timedelta(days=1)
     yesterday_str = yesterday.strftime(DATE_FORMAT)
     needs_fetch = False
     needs_save = False
-    fetch_duration = 8 # Fetch 8 days (yesterday + 7 future)
+    fetch_duration = 8  # Fetch 8 days (yesterday + 7 future)
 
-    for i in range(-1, fetch_duration -1): # -1 (yesterday) to 6 (today+6)
+    for i in range(-1, fetch_duration - 1):  # -1 (yesterday) to 6 (today+6)
         check_date = today + timedelta(days=i)
         check_date_str = check_date.strftime(DATE_FORMAT)
         if check_date_str not in cache.get(harbor_id, {}):
             _LOGGER.info(
                 "Marées France: Missing tide data for %s on %s. "
                 "Triggering full %d-day fetch.",
-                harbor_id, check_date_str, fetch_duration
+                harbor_id,
+                check_date_str,
+                fetch_duration,
             )
             needs_fetch = True
             break
@@ -573,16 +625,23 @@
         if fetch_successful:
             _LOGGER.info(
                 "Marées France: Successfully prefetched %d days of tide data for %s "
-                "starting %s.", fetch_duration, harbor_id, yesterday_str
+                "starting %s.",
+                fetch_duration,
+                harbor_id,
+                yesterday_str,
             )
             needs_save = True
         else:
-            _LOGGER.error("Marées France: Failed to prefetch tide data for %s.", harbor_id)
+            _LOGGER.error(
+                "Marées France: Failed to prefetch tide data for %s.", harbor_id
+            )
             return
     else:
         _LOGGER.info(
             "Marées France: Tide data cache is up to date for %s "
-            "(yesterday to today+%d).", harbor_id, fetch_duration - 2 # -1 for yesterday, -1 for 0-index
+            "(yesterday to today+%d).",
+            harbor_id,
+            fetch_duration - 2,  # -1 for yesterday, -1 for 0-index
         )
 
     if harbor_id in cache:
@@ -601,22 +660,27 @@
                 pruned_count += 1
                 _LOGGER.warning(
                     "Marées France: Removed tide cache entry with invalid date key: "
-                    "%s for %s", d_str, harbor_id
+                    "%s for %s",
+                    d_str,
+                    harbor_id,
                 )
         if pruned_count > 0:
             _LOGGER.info(
                 "Marées France: Pruned %d old tide data entries for %s.",
-                pruned_count, harbor_id
+                pruned_count,
+                harbor_id,
             )
         if not cache[harbor_id]:
             del cache[harbor_id]
             needs_save = True
 
-    if needs_save and not needs_fetch: # Fetch already saved
+    if needs_save and not needs_fetch:  # Fetch already saved
         await store.async_save(cache)
         _LOGGER.debug("Marées France: Saved pruned tides cache for %s", harbor_id)
 
-    _LOGGER.info("Marées France: Finished tide data prefetch check for harbor: %s", harbor_id)
+    _LOGGER.info(
+        "Marées France: Finished tide data prefetch check for harbor: %s", harbor_id
+    )
 
 
 async def async_handle_get_tides_data(call: ServiceCall) -> ServiceResponse:
@@ -644,16 +708,24 @@
     if not device_entry:
         raise HomeAssistantError(f"Device not found: {device_id}")
     if not device_entry.config_entries:
-        raise HomeAssistantError(f"Device {device_id} not associated with a config entry")
+        raise HomeAssistantError(
+            f"Device {device_id} not associated with a config entry"
+        )
     config_entry_id = next(iter(device_entry.config_entries))
     config_entry = hass.config_entries.async_get_entry(config_entry_id)
     if not config_entry or config_entry.domain != DOMAIN:
-        raise HomeAssistantError(f"Config entry {config_entry_id} not found or not for {DOMAIN}")
+        raise HomeAssistantError(
+            f"Config entry {config_entry_id} not found or not for {DOMAIN}"
+        )
     harbor_id = config_entry.data[CONF_HARBOR_ID]
 
-    _LOGGER.debug("Service call get_tides_data for device %s (harbor: %s)", device_id, harbor_id)
+    _LOGGER.debug(
+        "Service call get_tides_data for device %s (harbor: %s)", device_id, harbor_id
+    )
 
-    tides_store = Store[dict[str, dict[str, Any]]](hass, TIDES_STORAGE_VERSION, TIDES_STORAGE_KEY)
+    tides_store = Store[dict[str, dict[str, Any]]](
+        hass, TIDES_STORAGE_VERSION, TIDES_STORAGE_KEY
+    )
     cache = await tides_store.async_load() or {}
 
     harbor_data = cache.get(harbor_id, {})
@@ -664,13 +736,17 @@
         _LOGGER.warning(
             "Marées France: Invalid cache format for harbor '%s' (device: %s): "
             "Expected dict, got %s.",
-            harbor_id, device_id, type(harbor_data).__name__
+            harbor_id,
+            device_id,
+            type(harbor_data).__name__,
         )
     elif not harbor_data:
         data_valid = False
         _LOGGER.warning(
             "Marées France: No cached tide data found for harbor '%s' (device: %s) "
-            "in service call (empty cache entry).", harbor_id, device_id
+            "in service call (empty cache entry).",
+            harbor_id,
+            device_id,
         )
     else:
         for date_key, daily_tides in harbor_data.items():
@@ -679,27 +755,30 @@
                 _LOGGER.warning(
                     "Marées France: Invalid cache data for harbor '%s', date '%s' "
                     "(device: %s): Expected list, got %s.",
-                    harbor_id, date_key, device_id, type(daily_tides).__name__
+                    harbor_id,
+                    date_key,
+                    device_id,
+                    type(daily_tides).__name__,
                 )
                 break
 
     if not data_valid:
         return {
             "error": "invalid_or_missing_cache",
-            "message": f"Invalid or missing cached tide data found for harbor '{harbor_id}'"
+            "message": f"Invalid or missing cached tide data found for harbor '{harbor_id}'",
         }
 
     _LOGGER.debug(
         "Marées France: Returning valid cached tide data for harbor '%s' "
-        "(device: %s) via service call.", harbor_id, device_id
+        "(device: %s) via service call.",
+        harbor_id,
+        device_id,
     )
     return harbor_data
 
 
 async def async_check_and_prefetch_coefficients(
-    hass: HomeAssistant,
-    entry: ConfigEntry,
-    store: Store[dict[str, dict[str, Any]]]
+    hass: HomeAssistant, entry: ConfigEntry, store: Store[dict[str, dict[str, Any]]]
 ) -> None:
     """Check coefficient cache, prefetch missing data, and prune old entries.
 
@@ -714,7 +793,7 @@
     harbor_id = entry.data[CONF_HARBOR_ID]
     _LOGGER.info(
         "Marées France: Starting coefficient data prefetch check for harbor: %s",
-        harbor_id
+        harbor_id,
     )
     cache = await store.async_load() or {}
     today = date.today()
@@ -740,12 +819,15 @@
                 pruned_count += 1
                 _LOGGER.warning(
                     "Marées France: Removed coefficient cache entry with invalid "
-                    "date key: %s for %s", d_str, harbor_id
+                    "date key: %s for %s",
+                    d_str,
+                    harbor_id,
                 )
         if pruned_count > 0:
             _LOGGER.info(
                 "Marées France: Pruned %d old coefficient data entries for %s.",
-                pruned_count, harbor_id
+                pruned_count,
+                harbor_id,
             )
         if not cache[harbor_id]:
             del cache[harbor_id]
@@ -767,7 +849,8 @@
             _LOGGER.warning(
                 "Marées France: Found cached coefficient data for %s after missing "
                 "date %s. Inconsistency detected.",
-                check_date_str, first_missing_date.strftime(DATE_FORMAT)
+                check_date_str,
+                first_missing_date.strftime(DATE_FORMAT),
             )
         current_check_date += timedelta(days=1)
 
@@ -777,8 +860,10 @@
         _LOGGER.info(
             "Marées France: Missing coefficient data for %s starting %s (up to %s). "
             "Need to fetch %d days.",
-            harbor_id, fetch_start_date.strftime(DATE_FORMAT),
-            required_end_date.strftime(DATE_FORMAT), fetch_days
+            harbor_id,
+            fetch_start_date.strftime(DATE_FORMAT),
+            required_end_date.strftime(DATE_FORMAT),
+            fetch_days,
         )
 
         fetch_successful = await _async_fetch_and_store_coefficients(
@@ -788,32 +873,40 @@
             _LOGGER.info(
                 "Marées France: Successfully prefetched %d days of coefficient data "
                 "for %s starting %s.",
-                fetch_days, harbor_id, fetch_start_date.strftime(DATE_FORMAT)
+                fetch_days,
+                harbor_id,
+                fetch_start_date.strftime(DATE_FORMAT),
             )
-            needs_save = False # Fetch helper saved
+            needs_save = False  # Fetch helper saved
         else:
-            _LOGGER.error("Marées France: Failed to prefetch coefficient data for %s.", harbor_id)
-            if needs_save: # Save pruned state if fetch failed
+            _LOGGER.error(
+                "Marées France: Failed to prefetch coefficient data for %s.", harbor_id
+            )
+            if needs_save:  # Save pruned state if fetch failed
                 await store.async_save(cache)
                 _LOGGER.debug(
                     "Marées France: Saved pruned coefficients cache for %s "
-                    "after failed fetch.", harbor_id
+                    "after failed fetch.",
+                    harbor_id,
                 )
             return
     else:
         _LOGGER.info(
             "Marées France: Coefficient data cache is up to date for %s (from %s to %s).",
-            harbor_id, required_start_date.strftime(DATE_FORMAT),
-            required_end_date.strftime(DATE_FORMAT)
+            harbor_id,
+            required_start_date.strftime(DATE_FORMAT),
+            required_end_date.strftime(DATE_FORMAT),
         )
 
-    if needs_save: # Only pruning occurred
+    if needs_save:  # Only pruning occurred
         await store.async_save(cache)
-        _LOGGER.debug("Marées France: Saved pruned coefficients cache for %s", harbor_id)
+        _LOGGER.debug(
+            "Marées France: Saved pruned coefficients cache for %s", harbor_id
+        )
 
     _LOGGER.info(
         "Marées France: Finished coefficient data prefetch check for harbor: %s",
-        harbor_id
+        harbor_id,
     )
 
 
@@ -845,27 +938,38 @@
     dev_reg = dr.async_get(hass)
     device_entry = dev_reg.async_get(device_id)
     if not device_entry or not device_entry.config_entries:
-        raise HomeAssistantError(f"Device {device_id} not found or not linked to Marées France.")
+        raise HomeAssistantError(
+            f"Device {device_id} not found or not linked to Marées France."
+        )
     config_entry_id = next(iter(device_entry.config_entries))
     config_entry = hass.config_entries.async_get_entry(config_entry_id)
     if not config_entry or config_entry.domain != DOMAIN:
-        raise HomeAssistantError(f"Config entry {config_entry_id} not found or not for {DOMAIN}")
+        raise HomeAssistantError(
+            f"Config entry {config_entry_id} not found or not for {DOMAIN}"
+        )
     harbor_id = config_entry.data[CONF_HARBOR_ID]
 
     _LOGGER.debug(
         "Service call get_coefficients_data for device %s (harbor: %s), "
         "date: %s, days: %s",
-        device_id, harbor_id, req_date_str, req_days
+        device_id,
+        harbor_id,
+        req_date_str,
+        req_days,
     )
 
-    coeff_store = Store[dict[str, dict[str, Any]]](hass, COEFF_STORAGE_VERSION, COEFF_STORAGE_KEY)
+    coeff_store = Store[dict[str, dict[str, Any]]](
+        hass, COEFF_STORAGE_VERSION, COEFF_STORAGE_KEY
+    )
     cache = await coeff_store.async_load() or {}
     harbor_cache = cache.get(harbor_id, {})
 
     if not harbor_cache:
         _LOGGER.warning(
             "Marées France: No cached coefficient data found for harbor '%s' "
-            "(device: %s).", harbor_id, device_id
+            "(device: %s).",
+            harbor_id,
+            device_id,
         )
         return {}
 
@@ -891,7 +995,7 @@
     else:
         _LOGGER.debug(
             "Marées France: Returning all cached coefficient data for harbor '%s'.",
-            harbor_id
+            harbor_id,
         )
         return harbor_cache
 
@@ -902,8 +1006,12 @@
             results[current_date_str] = harbor_cache[current_date_str]
         current_date_iter += timedelta(days=1)
 
-    _LOGGER.debug("Marées France: Returning coefficient data for harbor '%s' from %s to %s.",
-                  harbor_id, start_date.strftime(DATE_FORMAT), end_date.strftime(DATE_FORMAT))
+    _LOGGER.debug(
+        "Marées France: Returning coefficient data for harbor '%s' from %s to %s.",
+        harbor_id,
+        start_date.strftime(DATE_FORMAT),
+        end_date.strftime(DATE_FORMAT),
+    )
     return results
 
 
@@ -932,15 +1040,20 @@
     Returns:
         True, indicating successful setup.
     """
+
     async def _setup_frontend(_event=None):
         """Inner function to register frontend modules."""
         await async_register_frontend_modules_when_ready(hass)
 
     if hass.state == CoreState.running:
-        _LOGGER.debug("Home Assistant already running, registering frontend modules immediately.")
+        _LOGGER.debug(
+            "Home Assistant already running, registering frontend modules immediately."
+        )
         await _setup_frontend()
     else:
-        _LOGGER.debug("Home Assistant not running yet, scheduling frontend module registration.")
+        _LOGGER.debug(
+            "Home Assistant not running yet, scheduling frontend module registration."
+        )
         hass.bus.async_listen_once(EVENT_HOMEASSISTANT_STARTED, _setup_frontend)
 
     return True
@@ -974,18 +1087,13 @@
     )
 
     coordinator = MareesFranceUpdateCoordinator(
-        hass,
-        entry,
-        tides_store,
-        coeff_store,
-        water_level_store
+        hass, entry, tides_store, coeff_store, water_level_store
     )
 
     await async_check_and_prefetch_coefficients(hass, entry, coeff_store)
     await async_check_and_prefetch_tides(hass, entry, tides_store)
     await async_check_and_prefetch_water_levels(hass, entry, water_level_store)
 
-
     hass.data.setdefault(DOMAIN, {})[entry.entry_id] = coordinator
 
     await hass.config_entries.async_forward_entry_setups(entry, PLATFORMS)
@@ -995,36 +1103,53 @@
 
     if not hass.services.has_service(DOMAIN, SERVICE_GET_WATER_LEVELS):
         hass.services.async_register(
-            DOMAIN, SERVICE_GET_WATER_LEVELS, async_handle_get_water_levels,
-            schema=SERVICE_GET_WATER_LEVELS_SCHEMA, supports_response=SupportsResponse.ONLY,
+            DOMAIN,
+            SERVICE_GET_WATER_LEVELS,
+            async_handle_get_water_levels,
+            schema=SERVICE_GET_WATER_LEVELS_SCHEMA,
+            supports_response=SupportsResponse.ONLY,
+        )
+        _LOGGER.debug(
+            "Marées France: Registered service: %s.%s", DOMAIN, SERVICE_GET_WATER_LEVELS
         )
-        _LOGGER.debug("Marées France: Registered service: %s.%s", DOMAIN, SERVICE_GET_WATER_LEVELS)
     if not hass.services.has_service(DOMAIN, SERVICE_GET_TIDES_DATA):
         hass.services.async_register(
-            DOMAIN, SERVICE_GET_TIDES_DATA, async_handle_get_tides_data,
-            schema=SERVICE_GET_TIDES_DATA_SCHEMA, supports_response=SupportsResponse.ONLY,
+            DOMAIN,
+            SERVICE_GET_TIDES_DATA,
+            async_handle_get_tides_data,
+            schema=SERVICE_GET_TIDES_DATA_SCHEMA,
+            supports_response=SupportsResponse.ONLY,
         )
-        _LOGGER.debug("Marées France: Registered service: %s.%s", DOMAIN, SERVICE_GET_TIDES_DATA)
+        _LOGGER.debug(
+            "Marées France: Registered service: %s.%s", DOMAIN, SERVICE_GET_TIDES_DATA
+        )
 
     if not hass.services.has_service(DOMAIN, SERVICE_GET_COEFFICIENTS_DATA):
         hass.services.async_register(
-            DOMAIN, SERVICE_GET_COEFFICIENTS_DATA, async_handle_get_coefficients_data,
-            schema=SERVICE_GET_COEFFICIENTS_DATA_SCHEMA, supports_response=SupportsResponse.ONLY,
+            DOMAIN,
+            SERVICE_GET_COEFFICIENTS_DATA,
+            async_handle_get_coefficients_data,
+            schema=SERVICE_GET_COEFFICIENTS_DATA_SCHEMA,
+            supports_response=SupportsResponse.ONLY,
         )
         _LOGGER.debug(
             "Marées France: Registered service: %s.%s",
-            DOMAIN, SERVICE_GET_COEFFICIENTS_DATA
+            DOMAIN,
+            SERVICE_GET_COEFFICIENTS_DATA,
         )
 
     if not hass.services.has_service(DOMAIN, SERVICE_REINITIALIZE_HARBOR_DATA):
         hass.services.async_register(
-            DOMAIN, SERVICE_REINITIALIZE_HARBOR_DATA, async_handle_reinitialize_harbor_data,
+            DOMAIN,
+            SERVICE_REINITIALIZE_HARBOR_DATA,
+            async_handle_reinitialize_harbor_data,
             schema=SERVICE_REINITIALIZE_HARBOR_DATA_SCHEMA,
             supports_response=SupportsResponse.NONE,
         )
         _LOGGER.debug(
             "Marées France: Registered service: %s.%s",
-            DOMAIN, SERVICE_REINITIALIZE_HARBOR_DATA
+            DOMAIN,
+            SERVICE_REINITIALIZE_HARBOR_DATA,
         )
 
     listeners = []
@@ -1032,30 +1157,46 @@
     async def _daily_water_level_prefetch_job(*_):
         _LOGGER.debug("Marées France: Running daily water level prefetch job.")
         await async_check_and_prefetch_water_levels(hass, entry, water_level_store)
+
     rand_wl_hour = random.randint(1, 5)
     rand_wl_min = random.randint(0, 59)
     _LOGGER.info(
         "Marées France: Scheduled daily water level prefetch check at %02d:%02d",
-        rand_wl_hour, rand_wl_min
+        rand_wl_hour,
+        rand_wl_min,
     )
-    listeners.append(async_track_time_change(
-        hass, _daily_water_level_prefetch_job, hour=rand_wl_hour, minute=rand_wl_min, second=0
-    ))
+    listeners.append(
+        async_track_time_change(
+            hass,
+            _daily_water_level_prefetch_job,
+            hour=rand_wl_hour,
+            minute=rand_wl_min,
+            second=0,
+        )
+    )
 
     async def _daily_tides_prefetch_job(*_):
         _LOGGER.debug("Marées France: Running daily tides prefetch job.")
         await async_check_and_prefetch_tides(hass, entry, tides_store)
+
     rand_t_hour = random.randint(1, 5)
     rand_t_min = random.randint(0, 59)
     while rand_t_hour == rand_wl_hour and rand_t_min == rand_wl_min:
         rand_t_min = random.randint(0, 59)
     _LOGGER.info(
         "Marées France: Scheduled daily tides prefetch check at %02d:%02d",
-        rand_t_hour, rand_t_min
+        rand_t_hour,
+        rand_t_min,
+    )
+    listeners.append(
+        async_track_time_change(
+            hass,
+            _daily_tides_prefetch_job,
+            hour=rand_t_hour,
+            minute=rand_t_min,
+            second=0,
+        )
     )
-    listeners.append(async_track_time_change(
-        hass, _daily_tides_prefetch_job, hour=rand_t_hour, minute=rand_t_min, second=0
-    ))
 
     async def _daily_coefficients_prefetch_job(*_):
         _LOGGER.debug("Marées France: Running daily coefficients prefetch job.")
@@ -1065,30 +1206,41 @@
         except Exception:
             _LOGGER.exception(
                 "Marées France: Error during scheduled coefficient prefetch job for %s",
-                entry.data.get(CONF_HARBOR_ID)
+                entry.data.get(CONF_HARBOR_ID),
             )
 
     rand_c_hour = random.randint(1, 5)
     rand_c_min = random.randint(0, 59)
-    while (rand_c_hour == rand_wl_hour and rand_c_min == rand_wl_min) or \
-          (rand_c_hour == rand_t_hour and rand_c_min == rand_t_min):
+    while (rand_c_hour == rand_wl_hour and rand_c_min == rand_wl_min) or (
+        rand_c_hour == rand_t_hour and rand_c_min == rand_t_min
+    ):
         rand_c_min = random.randint(0, 59)
-        if rand_c_min == rand_wl_min and rand_c_hour == rand_wl_hour: # pragma: no cover
+        if (
+            rand_c_min == rand_wl_min and rand_c_hour == rand_wl_hour
+        ):  # pragma: no cover
             continue
-        if rand_c_min == rand_t_min and rand_c_hour == rand_t_hour: # pragma: no cover
+        if rand_c_min == rand_t_min and rand_c_hour == rand_t_hour:  # pragma: no cover
             continue
     _LOGGER.info(
         "Marées France: Scheduled daily coefficients prefetch check at %02d:%02d",
-        rand_c_hour, rand_c_min
+        rand_c_hour,
+        rand_c_min,
     )
-    listeners.append(async_track_time_change(
-        hass, _daily_coefficients_prefetch_job, hour=rand_c_hour, minute=rand_c_min, second=0
-    ))
+    listeners.append(
+        async_track_time_change(
+            hass,
+            _daily_coefficients_prefetch_job,
+            hour=rand_c_hour,
+            minute=rand_c_min,
+            second=0,
+        )
+    )
 
     def _unload_listeners():
         _LOGGER.debug("Marées France: Removing daily prefetch listeners.")
         for remove_listener in listeners:
             remove_listener()
+
     entry.async_on_unload(_unload_listeners)
 
     return True
@@ -1113,9 +1265,12 @@
     unload_ok = await hass.config_entries.async_unload_platforms(entry, PLATFORMS)
 
     if unload_ok:
-        hass.data[DOMAIN].pop(entry.entry_id, None) # Use pop with default to avoid KeyError
+        hass.data[DOMAIN].pop(
+            entry.entry_id, None
+        )  # Use pop with default to avoid KeyError
         _LOGGER.debug(
-            "Marées France: Successfully unloaded Marées France entry: %s", entry.entry_id
+            "Marées France: Successfully unloaded Marées France entry: %s",
+            entry.entry_id,
         )
 
     return unload_ok
@@ -1138,6 +1293,7 @@
         "Marées France: Finished reloading Marées France entry: %s", entry.entry_id
     )
 
+
 async def async_remove_entry(hass: HomeAssistant, entry: ConfigEntry) -> None:
     """Handle removal of a config entry.
 
@@ -1153,14 +1309,18 @@
     if not harbor_id:
         _LOGGER.error(
             "Cannot remove cache for entry %s: Harbor ID not found in entry data.",
-            entry.entry_id
+            entry.entry_id,
         )
         return
 
-    _LOGGER.info("Removing cached data for harbor %s (entry: %s)", harbor_id, entry.entry_id)
+    _LOGGER.info(
+        "Removing cached data for harbor %s (entry: %s)", harbor_id, entry.entry_id
+    )
 
     stores_to_clean = {
-        "tides": Store[dict[str, dict[str, Any]]](hass, TIDES_STORAGE_VERSION, TIDES_STORAGE_KEY),
+        "tides": Store[dict[str, dict[str, Any]]](
+            hass, TIDES_STORAGE_VERSION, TIDES_STORAGE_KEY
+        ),
         "coefficients": Store[dict[str, dict[str, Any]]](
             hass, COEFF_STORAGE_VERSION, COEFF_STORAGE_KEY
         ),
@@ -1175,16 +1335,21 @@
             if harbor_id in cache_data:
                 del cache_data[harbor_id]
                 await store_instance.async_save(cache_data)
-                _LOGGER.debug("Removed %s cache data for harbor %s.", store_name, harbor_id)
+                _LOGGER.debug(
+                    "Removed %s cache data for harbor %s.", store_name, harbor_id
+                )
             else:
                 _LOGGER.debug(
                     "No %s cache data found for harbor %s to remove.",
-                    store_name, harbor_id
+                    store_name,
+                    harbor_id,
                 )
         except Exception as e:
             _LOGGER.exception(
                 "Error removing %s cache data for harbor %s: %s",
-                store_name, harbor_id, e
+                store_name,
+                harbor_id,
+                e,
             )
 
     _LOGGER.info("Finished removing cached data for harbor %s.", harbor_id)

--- custom_components/marees_france/api_helpers.py
+++ custom_components/marees_france/api_helpers.py
@@ -57,11 +57,13 @@
     await asyncio.sleep(API_REQUEST_DELAY)
     _LOGGER.debug(
         "Marées France Helper: Preparing to fetch %s for %s from %s",
-        data_type, harbor_id, url
+        data_type,
+        harbor_id,
+        url,
     )
 
     for attempt in range(max_retries):
-        current_delay = initial_delay * (2 ** attempt)
+        current_delay = initial_delay * (2**attempt)
         try:
             async with asyncio.timeout(timeout):
                 response = await session.get(url, headers=headers)
@@ -69,7 +71,10 @@
                 data = await response.json()
                 _LOGGER.debug(
                     "Marées France Helper: Successfully fetched %s for %s (attempt %d/%d)",
-                    data_type, harbor_id, attempt + 1, max_retries
+                    data_type,
+                    harbor_id,
+                    attempt + 1,
+                    max_retries,
                 )
                 return data
 
@@ -77,26 +82,45 @@
             _LOGGER.warning(
                 "Marées France Helper: Timeout fetching %s for %s (attempt %d/%d). "
                 "Retrying in %ds...",
-                data_type, harbor_id, attempt + 1, max_retries, current_delay
+                data_type,
+                harbor_id,
+                attempt + 1,
+                max_retries,
+                current_delay,
             )
         except aiohttp.ClientResponseError as err:
             _LOGGER.warning(
                 "Marées France Helper: HTTP error %s fetching %s for %s (attempt %d/%d): %s. "
                 "Retrying in %ds...",
-                err.status, data_type, harbor_id, attempt + 1, max_retries,
-                err.message, current_delay
+                err.status,
+                data_type,
+                harbor_id,
+                attempt + 1,
+                max_retries,
+                err.message,
+                current_delay,
             )
         except aiohttp.ClientError as err:
             _LOGGER.warning(
                 "Marées France Helper: Client error fetching %s for %s (attempt %d/%d): %s. "
                 "Retrying in %ds...",
-                data_type, harbor_id, attempt + 1, max_retries, err, current_delay
+                data_type,
+                harbor_id,
+                attempt + 1,
+                max_retries,
+                err,
+                current_delay,
             )
         except Exception as err:
             _LOGGER.warning(
                 "Marées France Helper: Unexpected error fetching %s for %s (attempt %d/%d): %s. "
                 "Retrying in %ds...",
-                data_type, harbor_id, attempt + 1, max_retries, err, current_delay
+                data_type,
+                harbor_id,
+                attempt + 1,
+                max_retries,
+                err,
+                current_delay,
             )
 
         if attempt < max_retries - 1:
@@ -104,7 +128,9 @@
         else:
             _LOGGER.error(
                 "Marées France Helper: Failed to fetch %s for %s after %d attempts.",
-                data_type, harbor_id, max_retries
+                data_type,
+                harbor_id,
+                max_retries,
             )
     return None
 
@@ -138,23 +164,23 @@
         headers=HEADERS,
         timeout=timeout_seconds,
         harbor_id=harbor_name,
-        data_type="water levels"
+        data_type="water levels",
     )
 
     if data is None:
         return None
 
     valid_structure = (
-        isinstance(data, dict) and
-        date_str in data and
-        isinstance(data[date_str], list)
+        isinstance(data, dict) and date_str in data and isinstance(data[date_str], list)
     )
 
     if not valid_structure:
         _LOGGER.error(
             "Marées France Helper: Fetched water level data for %s on %s has unexpected "
             "structure or is missing the date key. Discarding. Data: %s",
-            harbor_name, date_str, data
+            harbor_name,
+            date_str,
+            data,
         )
         return None
 
@@ -210,13 +236,15 @@
         headers=HEADERS,
         timeout=timeout_seconds,
         harbor_id=harbor_id,
-        data_type="tides"
+        data_type="tides",
     )
 
     if fetched_data_dict is None or not isinstance(fetched_data_dict, dict):
         _LOGGER.error(
             "Marées France Helper: Failed to fetch or received invalid format for "
-            "tide data for %s starting %s.", harbor_id, start_date_str
+            "tide data for %s starting %s.",
+            harbor_id,
+            start_date_str,
         )
         return False
 
@@ -226,11 +254,14 @@
             cache[harbor_id][day_str] = tides
             _LOGGER.debug(
                 "Marées France Helper: Updated tide cache for %s on %s",
-                harbor_id, day_str
+                harbor_id,
+                day_str,
             )
 
         await store.async_save(cache)
-        _LOGGER.debug("Marées France Helper: Saved updated tides cache for %s", harbor_id)
+        _LOGGER.debug(
+            "Marées France Helper: Saved updated tides cache for %s", harbor_id
+        )
         return True
     except Exception:
         _LOGGER.exception(
@@ -264,7 +295,9 @@
         of days, False otherwise.
     """
     start_date_str = start_date.strftime(DATE_FORMAT)
-    url = COEFF_URL_TEMPLATE.format(harbor_name=harbor_id, date=start_date_str, days=days)
+    url = COEFF_URL_TEMPLATE.format(
+        harbor_name=harbor_id, date=start_date_str, days=days
+    )
     session = async_get_clientsession(hass)
     timeout_seconds = 60
 
@@ -274,14 +307,16 @@
         headers=HEADERS,
         timeout=timeout_seconds,
         harbor_id=harbor_id,
-        data_type="coefficients"
+        data_type="coefficients",
     )
 
     if fetched_data_list is None or not isinstance(fetched_data_list, list):
         _LOGGER.error(
             "Marées France Helper: Failed to fetch or received invalid format for "
             "coefficient data for %s starting %s (%d days).",
-            harbor_id, start_date_str, days
+            harbor_id,
+            start_date_str,
+            days,
         )
         return False
 
@@ -302,22 +337,28 @@
                         for coeff_item in daily_coeffs:
                             if isinstance(coeff_item, str):
                                 parsed_coeffs.append(coeff_item)
-                            elif (isinstance(coeff_item, list)
-                                  and len(coeff_item) == 1
-                                  and isinstance(coeff_item[0], str)):
+                            elif (
+                                isinstance(coeff_item, list)
+                                and len(coeff_item) == 1
+                                and isinstance(coeff_item[0], str)
+                            ):
                                 parsed_coeffs.append(coeff_item[0])
                             else:
                                 _LOGGER.warning(
                                     "Marées France Helper: Unexpected item format within daily "
                                     "coefficients for %s on %s: %s. Skipping item.",
-                                    harbor_id, day_str, coeff_item
+                                    harbor_id,
+                                    day_str,
+                                    coeff_item,
                                 )
 
                         if parsed_coeffs:
                             cache[harbor_id][day_str] = parsed_coeffs
                             _LOGGER.debug(
                                 "Marées France Helper: Updated coefficient cache for %s on %s: %s",
-                                harbor_id, day_str, parsed_coeffs
+                                harbor_id,
+                                day_str,
+                                parsed_coeffs,
                             )
                         else:
                             _LOGGER.warning(
@@ -331,7 +372,9 @@
                         _LOGGER.warning(
                             "Marées France Helper: Unexpected format for daily coefficients "
                             "container for %s on %s: %s. Skipping day.",
-                            harbor_id, day_str, daily_coeffs
+                            harbor_id,
+                            day_str,
+                            daily_coeffs,
                         )
                     processed_days_count += 1
             if processed_days_count >= days:
@@ -341,20 +384,27 @@
             await store.async_save(cache)
             _LOGGER.debug(
                 "Marées France Helper: Saved updated coefficients cache for %s "
-                "after processing %d days.", harbor_id, processed_days_count
+                "after processing %d days.",
+                harbor_id,
+                processed_days_count,
             )
             return True
 
         _LOGGER.error(
             "Marées France Helper: Processed %d days of coefficient data, but expected %d "
             "for %s starting %s. API data might be incomplete or parsing failed.",
-            processed_days_count, days, harbor_id, start_date_str
+            processed_days_count,
+            days,
+            harbor_id,
+            start_date_str,
         )
-        if processed_days_count > 0: # Save partial data if any was processed
+        if processed_days_count > 0:  # Save partial data if any was processed
             await store.async_save(cache)
             _LOGGER.debug(
                 "Marées France Helper: Saved partially updated coefficients cache for %s "
-                "(%d days processed).", harbor_id, processed_days_count
+                "(%d days processed).",
+                harbor_id,
+                processed_days_count,
             )
         return False
 
@@ -364,6 +414,6 @@
             "starting %s (%d days)",
             harbor_id,
             start_date_str,
-            days
+            days,
         )
         return False

--- custom_components/marees_france/config_flow.py
+++ custom_components/marees_france/config_flow.py
@@ -11,7 +11,7 @@
 from homeassistant.exceptions import HomeAssistantError
 from homeassistant.helpers.aiohttp_client import async_get_clientsession
 
-from . import fetch_harbors # fetch_harbors is defined in __init__.py
+from . import fetch_harbors  # fetch_harbors is defined in __init__.py
 from .const import (
     CONF_HARBOR_ID,
     CONF_HARBOR_NAME,
@@ -27,7 +27,9 @@
     """Error to indicate we cannot connect to the SHOM API."""
 
 
-class InvalidAuth(HomeAssistantError): # Not currently used, but good for future API changes.
+class InvalidAuth(
+    HomeAssistantError
+):  # Not currently used, but good for future API changes.
     """Error to indicate there is invalid authentication."""
 
 
@@ -64,14 +66,17 @@
             except CannotConnect as err:
                 _LOGGER.error("Failed to connect to SHOM API to fetch harbors: %s", err)
                 return self.async_abort(reason="cannot_connect")
-            except Exception as err: # pylint: disable=broad-except
+            except Exception as err:  # pylint: disable=broad-except
                 _LOGGER.exception("Unexpected error fetching harbors: %s", err)
                 return self.async_abort(reason="unknown")
 
         if user_input is not None:
             selected_harbor_id = user_input[CONF_HARBOR_ID]
             # Ensure _harbors_cache is not None before accessing
-            if self._harbors_cache is None or selected_harbor_id not in self._harbors_cache:
+            if (
+                self._harbors_cache is None
+                or selected_harbor_id not in self._harbors_cache
+            ):
                 errors["base"] = "invalid_harbor"
             else:
                 await self.async_set_unique_id(selected_harbor_id.lower())
@@ -81,7 +86,10 @@
 
                 return self.async_create_entry(
                     title=f"{INTEGRATION_NAME} - {harbor_name}",
-                    data={CONF_HARBOR_ID: selected_harbor_id, CONF_HARBOR_NAME: harbor_name}
+                    data={
+                        CONF_HARBOR_ID: selected_harbor_id,
+                        CONF_HARBOR_NAME: harbor_name,
+                    },
                 )
 
         harbor_options = {

--- custom_components/marees_france/const.py
+++ custom_components/marees_france/const.py
@@ -31,10 +31,12 @@
 
 # --- Configuration Constants ---
 CONF_HARBOR_ID: Final[str] = "harbor_id"
-CONF_HARBOR_NAME: Final[str] = "harbor_name" # Used in config entry, not directly by user
+CONF_HARBOR_NAME: Final[str] = (
+    "harbor_name"  # Used in config entry, not directly by user
+)
 
 # --- Default Values ---
-DEFAULT_HARBOR: Final[str] = "PORNICHET" # Default harbor for config flow
+DEFAULT_HARBOR: Final[str] = "PORNICHET"  # Default harbor for config flow
 
 # --- API Configuration ---
 HARBORSURL: Final[str] = (
@@ -59,24 +61,32 @@
     "User-Agent": (
         "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
         "AppleWebKit/537.36 (KHTML, like Gecko) "
-        "Chrome/109.0.0.0 Safari/537.36" # Common User-Agent
+        "Chrome/109.0.0.0 Safari/537.36"  # Common User-Agent
     ),
 }
 API_REQUEST_DELAY: Final[float] = 0.2  # Delay in seconds between API requests
 
 # --- Data Attributes & Keys ---
-ATTR_DATA: Final[str] = "data" # General data attribute
-ATTR_NEXT_TIDE: Final[str] = "next" # Key for next tide information
-ATTR_PREVIOUS_TIDE: Final[str] = "previous" # Key for previous tide information
-ATTR_HARBOR_NAME: Final[str] = "harbor_name" # Attribute for harbor name in sensor state
-ATTR_DATE: Final[str] = "date" # Attribute/key for date
-ATTR_COEFFICIENT: Final[str] = "coefficient" # Attribute for tide coefficient
-ATTR_TIDE_TREND: Final[str] = "tide_trend" # Attribute for tide trend (e.g., rising, falling)
-ATTR_STARTING_HEIGHT: Final[str] = "starting_height" # Attribute for tide starting height
-ATTR_FINISHED_HEIGHT: Final[str] = "finished_height" # Attribute for tide finished height
-ATTR_STARTING_TIME: Final[str] = "starting_time" # Attribute for tide starting time
-ATTR_FINISHED_TIME: Final[str] = "finished_time" # Attribute for tide finished time
-ATTR_CURRENT_HEIGHT: Final[str] = "current_height" # Attribute for current water height
+ATTR_DATA: Final[str] = "data"  # General data attribute
+ATTR_NEXT_TIDE: Final[str] = "next"  # Key for next tide information
+ATTR_PREVIOUS_TIDE: Final[str] = "previous"  # Key for previous tide information
+ATTR_HARBOR_NAME: Final[str] = (
+    "harbor_name"  # Attribute for harbor name in sensor state
+)
+ATTR_DATE: Final[str] = "date"  # Attribute/key for date
+ATTR_COEFFICIENT: Final[str] = "coefficient"  # Attribute for tide coefficient
+ATTR_TIDE_TREND: Final[str] = (
+    "tide_trend"  # Attribute for tide trend (e.g., rising, falling)
+)
+ATTR_STARTING_HEIGHT: Final[str] = (
+    "starting_height"  # Attribute for tide starting height
+)
+ATTR_FINISHED_HEIGHT: Final[str] = (
+    "finished_height"  # Attribute for tide finished height
+)
+ATTR_STARTING_TIME: Final[str] = "starting_time"  # Attribute for tide starting time
+ATTR_FINISHED_TIME: Final[str] = "finished_time"  # Attribute for tide finished time
+ATTR_CURRENT_HEIGHT: Final[str] = "current_height"  # Attribute for current water height
 
 # --- Storage Keys and Versions ---
 WATERLEVELS_STORAGE_KEY: Final[str] = f"{DOMAIN}_water_levels_cache"
@@ -87,11 +97,11 @@
 COEFF_STORAGE_VERSION: Final[int] = 1
 
 # --- Tide Types & Thresholds ---
-TIDE_HIGH: Final[str] = "tide.high" # Internal representation for high tide
-TIDE_LOW: Final[str] = "tide.low"   # Internal representation for low tide
-TIDE_NONE: Final[str] = "tide.none" # Internal representation for no current tide event
-SPRING_TIDE_THRESHOLD: Final[int] = 100 # Coefficient threshold for spring tide
-NEAP_TIDE_THRESHOLD: Final[int] = 40   # Coefficient threshold for neap tide
+TIDE_HIGH: Final[str] = "tide.high"  # Internal representation for high tide
+TIDE_LOW: Final[str] = "tide.low"  # Internal representation for low tide
+TIDE_NONE: Final[str] = "tide.none"  # Internal representation for no current tide event
+SPRING_TIDE_THRESHOLD: Final[int] = 100  # Coefficient threshold for spring tide
+NEAP_TIDE_THRESHOLD: Final[int] = 40  # Coefficient threshold for neap tide
 
 # --- Translation Keys for Sensor State (matches strings.json) ---
 STATE_HIGH_TIDE: Final[str] = "high_tide"
@@ -121,4 +131,4 @@
         "version": INTEGRATION_VERSION,
     },
 ]
-URL_BASE: Final[str] = "/marees-france" # Base URL for frontend resources
+URL_BASE: Final[str] = "/marees-france"  # Base URL for frontend resources

--- custom_components/marees_france/coordinator.py
+++ custom_components/marees_france/coordinator.py
@@ -93,8 +93,14 @@
         cache_full: dict[str, dict[str, Any]],
         data_type: str,
         fetch_function: Callable[
-            [HomeAssistant, Store[dict[str, dict[str, Any]]], dict[str, dict[str, Any]], str, Any],
-            Coroutine[Any, Any, bool | Any | None]
+            [
+                HomeAssistant,
+                Store[dict[str, dict[str, Any]]],
+                dict[str, dict[str, Any]],
+                str,
+                Any,
+            ],
+            Coroutine[Any, Any, bool | Any | None],
         ],
         fetch_args: tuple,
     ) -> tuple[dict[str, dict[str, Any]], dict[str, Any]]:
@@ -127,26 +133,35 @@
             _LOGGER.warning(
                 "Marées France Coordinator: Invalid cache format for %s harbor '%s': "
                 "Expected dict, got %s.",
-                data_type, self.harbor_id, type(harbor_cache).__name__
+                data_type,
+                self.harbor_id,
+                type(harbor_cache).__name__,
             )
             needs_repair = True
-        elif not harbor_cache and data_type != "water_levels": # Allow empty water_levels initially
+        elif (
+            not harbor_cache and data_type != "water_levels"
+        ):  # Allow empty water_levels initially
             _LOGGER.warning(
                 "Marées France Coordinator: Empty %s cache entry found for harbor '%s'.",
-                data_type, self.harbor_id
+                data_type,
+                self.harbor_id,
             )
             needs_repair = True
         else:
             # Specific validation for data structures within the harbor_cache
             if data_type == "water_levels":
                 for date_key, daily_data in harbor_cache.items():
-                    if not isinstance(daily_data, dict) or \
-                       date_key not in daily_data or \
-                       not isinstance(daily_data.get(date_key), list):
+                    if (
+                        not isinstance(daily_data, dict)
+                        or date_key not in daily_data
+                        or not isinstance(daily_data.get(date_key), list)
+                    ):
                         _LOGGER.warning(
                             "Marées France Coordinator: Invalid %s cache structure for "
                             "harbor '%s', date '%s'.",
-                            data_type, self.harbor_id, date_key
+                            data_type,
+                            self.harbor_id,
+                            date_key,
                         )
                         needs_repair = True
                         break
@@ -156,7 +171,10 @@
                         _LOGGER.warning(
                             "Marées France Coordinator: Invalid %s cache data for harbor '%s', "
                             "date '%s': Expected list, got %s.",
-                            data_type, self.harbor_id, date_key, type(daily_data).__name__
+                            data_type,
+                            self.harbor_id,
+                            date_key,
+                            type(daily_data).__name__,
                         )
                         needs_repair = True
                         break
@@ -165,7 +183,8 @@
             _LOGGER.warning(
                 "Marées France Coordinator: Invalid or empty %s cache detected "
                 "for %s. Attempting repair.",
-                data_type, self.harbor_id
+                data_type,
+                self.harbor_id,
             )
             try:
                 if self.harbor_id in cache_full:
@@ -173,12 +192,15 @@
                 await store.async_save(cache_full)
                 _LOGGER.info(
                     "Marées France Coordinator: Removed invalid %s cache entry for %s.",
-                    data_type, self.harbor_id
+                    data_type,
+                    self.harbor_id,
                 )
 
                 _LOGGER.info(
                     "Marées France Coordinator: Triggering immediate fetch for %s data "
-                    "for %s.", data_type, self.harbor_id
+                    "for %s.",
+                    data_type,
+                    self.harbor_id,
                 )
                 fetch_successful = await fetch_function(
                     self.hass, store, cache_full, self.harbor_id, *fetch_args
@@ -187,25 +209,29 @@
                 if fetch_successful:
                     _LOGGER.info(
                         "Marées France Coordinator: Successfully re-fetched %s data for %s "
-                        "after cache repair.", data_type, self.harbor_id
+                        "after cache repair.",
+                        data_type,
+                        self.harbor_id,
                     )
                     cache_full = await store.async_load() or {}
                     harbor_cache = cache_full.get(self.harbor_id, {})
                 else:
                     _LOGGER.error(
                         "Marées France Coordinator: Failed to re-fetch %s data for %s "
-                        "after cache repair.", data_type, self.harbor_id
+                        "after cache repair.",
+                        data_type,
+                        self.harbor_id,
                     )
                     harbor_cache = {}
             except Exception:
                 _LOGGER.exception(
                     "Marées France Coordinator: Error during %s cache repair for %s.",
-                    data_type, self.harbor_id
+                    data_type,
+                    self.harbor_id,
                 )
                 harbor_cache = {}
         return cache_full, harbor_cache
 
-
     async def _async_update_data(self) -> dict[str, Any]:
         """Fetch and process tide, coefficient, and water level data.
 
@@ -220,7 +246,9 @@
         Raises:
             UpdateFailed: If essential data (like tides) cannot be loaded or fetched.
         """
-        _LOGGER.debug("Marées France Coordinator: Starting update cycle for %s", self.harbor_id)
+        _LOGGER.debug(
+            "Marées France Coordinator: Starting update cycle for %s", self.harbor_id
+        )
 
         try:
             tides_cache_full = await self.tides_store.async_load() or {}
@@ -228,7 +256,8 @@
             water_level_cache_full = await self.water_level_store.async_load() or {}
         except Exception as e:
             _LOGGER.exception(
-                "Marées France Coordinator: Failed to load cache stores for %s", self.harbor_id
+                "Marées France Coordinator: Failed to load cache stores for %s",
+                self.harbor_id,
             )
             raise UpdateFailed(f"Failed to load cache: {e}") from e
 
@@ -238,30 +267,37 @@
         tide_fetch_duration = 8
 
         tides_cache_full, harbor_tides_cache = await self._validate_and_repair_cache(
-            self.tides_store, tides_cache_full, "tides",
-            _async_fetch_and_store_tides, (yesterday_str, tide_fetch_duration)
+            self.tides_store,
+            tides_cache_full,
+            "tides",
+            _async_fetch_and_store_tides,
+            (yesterday_str, tide_fetch_duration),
         )
 
         first_day_of_current_month = today.replace(day=1)
         coeff_fetch_days = 365
         coeff_cache_full, harbor_coeff_cache = await self._validate_and_repair_cache(
-            self.coeff_store, coeff_cache_full, "coefficients",
+            self.coeff_store,
+            coeff_cache_full,
+            "coefficients",
             _async_fetch_and_store_coefficients,
-            (first_day_of_current_month, coeff_fetch_days)
+            (first_day_of_current_month, coeff_fetch_days),
         )
 
         # Validate water levels, but repair fetch only targets today if triggered by validation.
         # Full prefetch is handled by scheduled jobs in __init__.py.
         _, harbor_water_level_cache_validated = await self._validate_and_repair_cache(
-            self.water_level_store, water_level_cache_full, "water_levels",
-            _async_fetch_and_store_water_level, (today_str,)
+            self.water_level_store,
+            water_level_cache_full,
+            "water_levels",
+            _async_fetch_and_store_water_level,
+            (today_str,),
         )
         # Reload full water level cache as repair might have modified it.
         water_level_cache_full = await self.water_level_store.async_load() or {}
         # Use the potentially repaired harbor-specific cache for today's data.
         harbor_water_level_cache = water_level_cache_full.get(self.harbor_id, {})
 
-
         future_window_days = 366
         tides_data_for_parser = {}
         coeff_data_for_parser = {}
@@ -271,11 +307,15 @@
             _LOGGER.info(
                 "Marées France Coordinator: Today's (%s) water level data missing "
                 "from cache for %s post-validation. Attempting fetch...",
-                today_str, self.harbor_id
+                today_str,
+                self.harbor_id,
             )
             fetched_data = await _async_fetch_and_store_water_level(
-                self.hass, self.water_level_store, water_level_cache_full,
-                self.harbor_id, today_str
+                self.hass,
+                self.water_level_store,
+                water_level_cache_full,
+                self.harbor_id,
+                today_str,
             )
             if fetched_data:
                 _LOGGER.info(
@@ -294,32 +334,41 @@
             check_date = today + timedelta(days=i)
             check_date_str = check_date.strftime(DATE_FORMAT)
             if check_date_str in harbor_tides_cache:
-                tides_data_for_parser[check_date_str] = harbor_tides_cache[check_date_str]
+                tides_data_for_parser[check_date_str] = harbor_tides_cache[
+                    check_date_str
+                ]
 
         for i in range(future_window_days + 1):  # Today for coeffs
             check_date = today + timedelta(days=i)
             check_date_str = check_date.strftime(DATE_FORMAT)
             if check_date_str in harbor_coeff_cache:
-                coeff_data_for_parser[check_date_str] = harbor_coeff_cache[check_date_str]
+                coeff_data_for_parser[check_date_str] = harbor_coeff_cache[
+                    check_date_str
+                ]
 
         _LOGGER.debug(
             "Marées France Coordinator: Loaded %d days of tide data and %d days of "
             "coeff data for parser post-validation.",
-            len(tides_data_for_parser), len(coeff_data_for_parser)
+            len(tides_data_for_parser),
+            len(coeff_data_for_parser),
         )
 
         if not tides_data_for_parser:
             _LOGGER.error(
                 "Marées France Coordinator: No tide data available for %s even after "
-                "validation/repair.", self.harbor_id
+                "validation/repair.",
+                self.harbor_id,
             )
             raise UpdateFailed(
                 f"No tide data available for {self.harbor_id} after validation/repair."
             )
-        if not coeff_data_for_parser:  # Allow proceeding without coeffs, but log warning
+        if (
+            not coeff_data_for_parser
+        ):  # Allow proceeding without coeffs, but log warning
             _LOGGER.warning(
                 "Marées France Coordinator: No coefficient data available for %s after "
-                "validation/repair. Proceeding without it.", self.harbor_id
+                "validation/repair. Proceeding without it.",
+                self.harbor_id,
             )
 
         try:
@@ -337,19 +386,20 @@
 
             _LOGGER.debug(
                 "Marées France Coordinator: Calling _parse_tide_data with "
-                "water_level_data_for_parser: %s", water_level_data_for_parser
+                "water_level_data_for_parser: %s",
+                water_level_data_for_parser,
             )
             return await self._parse_tide_data(
                 tides_data_for_parser,
                 coeff_data_for_parser,
                 water_level_data_for_parser,
-                translation_high, # Though unused, kept for signature consistency
-                translation_low   # Though unused, kept for signature consistency
+                translation_high,  # Though unused, kept for signature consistency
+                translation_low,  # Though unused, kept for signature consistency
             )
         except Exception as err:
             _LOGGER.exception(
                 "Marées France Coordinator: Unexpected error processing data for %s",
-                self.harbor_id
+                self.harbor_id,
             )
             raise UpdateFailed(f"Error processing data: {err}") from err
 
@@ -358,7 +408,7 @@
         tides_raw_data: dict[str, list[list[str]]],
         coeff_raw_data: dict[str, list[str]],
         water_level_raw_data: dict | None,
-        _translation_high: str, # Parameter kept for signature, but not used directly
+        _translation_high: str,  # Parameter kept for signature, but not used directly
         _translation_low: str,  # Parameter kept for signature, but not used directly
     ) -> dict[str, Any]:
         """Parse raw tide, coefficient, and water level data into a structured format.
@@ -404,7 +454,9 @@
                 if not isinstance(tide_info, (list, tuple)) or len(tide_info) != 4:
                     _LOGGER.warning(
                         "Marées France Coordinator: Skipping invalid tide_info format "
-                        "for %s: %s", day_str, tide_info
+                        "for %s: %s",
+                        day_str,
+                        tide_info,
                     )
                     continue
                 tide_type, time_str, height_str, coeff_str = tide_info
@@ -419,7 +471,8 @@
                 except ValueError:
                     _LOGGER.warning(
                         "Marées France Coordinator: Could not parse datetime: %s %s",
-                        day_str, time_str
+                        day_str,
+                        time_str,
                     )
                     continue
 
@@ -432,8 +485,10 @@
                     "datetime_utc": tide_dt_utc.isoformat(),
                     "date_local": day_str,
                     "translated_type": (  # Convert dots to underscores for frontend
-                        "tide_high" if tide_type == TIDE_HIGH
-                        else "tide_low" if tide_type == TIDE_LOW
+                        "tide_high"
+                        if tide_type == TIDE_HIGH
+                        else "tide_low"
+                        if tide_type == TIDE_LOW
                         else "Unknown"
                     ),
                 }
@@ -447,7 +502,8 @@
                 if day_coeffs and isinstance(day_coeffs, list):
                     try:
                         valid_coeffs_int = [
-                            int(c) for c in day_coeffs
+                            int(c)
+                            for c in day_coeffs
                             if isinstance(c, str) and c.isdigit()
                         ]
                         if valid_coeffs_int:
@@ -455,7 +511,9 @@
                             _LOGGER.debug(
                                 "Marées France Coordinator: Assigned max daily coeff %s to "
                                 "tide on %s %s",
-                                tide["coefficient"], tide["date_local"], tide["time_local"]
+                                tide["coefficient"],
+                                tide["date_local"],
+                                tide["time_local"],
                             )
                         else:
                             tide["coefficient"] = None
@@ -463,7 +521,9 @@
                         _LOGGER.warning(
                             "Marées France Coordinator: Error processing daily coefficients "
                             "for %s %s: %s",
-                            tide["date_local"], tide["time_local"], day_coeffs
+                            tide["date_local"],
+                            tide["time_local"],
+                            day_coeffs,
                         )
                         tide["coefficient"] = None
                 else:
@@ -484,7 +544,8 @@
             next_tide_event = all_tides_flat[now_tide_index]
             next_starting_height = (
                 all_tides_flat[now_tide_index - 1]["height"]
-                if now_tide_index > 0 else None
+                if now_tide_index > 0
+                else None
             )
             next_data = {
                 ATTR_TIDE_TREND: next_tide_event["translated_type"],
@@ -499,7 +560,8 @@
                 previous_tide_event = all_tides_flat[now_tide_index - 1]
                 previous_starting_height = (
                     all_tides_flat[now_tide_index - 2]["height"]
-                    if now_tide_index > 1 else None
+                    if now_tide_index > 1
+                    else None
                 )
                 previous_data = {
                     ATTR_TIDE_TREND: previous_tide_event["translated_type"],
@@ -510,7 +572,9 @@
                     ATTR_COEFFICIENT: previous_tide_event["coefficient"],
                 }
 
-                tide_status = "rising" if previous_tide_event["type"] == TIDE_LOW else "falling"
+                tide_status = (
+                    "rising" if previous_tide_event["type"] == TIDE_LOW else "falling"
+                )
                 now_data = {
                     ATTR_TIDE_TREND: tide_status,
                     ATTR_STARTING_TIME: previous_tide_event["datetime_utc"],
@@ -523,7 +587,8 @@
         current_water_height = None
         _LOGGER.debug(
             "Marées France Coordinator: Checking water level data for current height "
-            "calculation. Received data: %s", water_level_raw_data
+            "calculation. Received data: %s",
+            water_level_raw_data,
         )
         water_levels = None
         today_str_key = date.today().strftime(DATE_FORMAT)
@@ -533,14 +598,17 @@
         ):
             _LOGGER.debug(
                 "Marées France Coordinator: Extracting water levels using key '%s'.",
-                today_str_key
+                today_str_key,
             )
             water_levels = water_level_raw_data[today_str_key]
-        elif water_level_raw_data is not None:  # Log if data is present but not in expected format
+        elif (
+            water_level_raw_data is not None
+        ):  # Log if data is present but not in expected format
             _LOGGER.warning(
                 "Marées France Coordinator: Received water level data, but it's not a "
                 "dictionary or lacks the expected key '%s'. Data: %s",
-                today_str_key, water_level_raw_data
+                today_str_key,
+                water_level_raw_data,
             )
 
         if water_levels:
@@ -564,12 +632,18 @@
                         if diff < min_diff:
                             min_diff = diff
                             closest_entry = entry
-                    except (ValueError, TypeError, pytz.exceptions.AmbiguousTimeError,
-                            pytz.exceptions.NonExistentTimeError) as e:
+                    except (
+                        ValueError,
+                        TypeError,
+                        pytz.exceptions.AmbiguousTimeError,
+                        pytz.exceptions.NonExistentTimeError,
+                    ) as e:
                         _LOGGER.warning(
                             "Marées France Coordinator: Skipping water level entry due to "
                             "parsing/timezone error for %s: %s (%s)",
-                            entry, e.__class__.__name__, e
+                            entry,
+                            e.__class__.__name__,
+                            e,
                         )
                         continue
             if closest_entry:
@@ -579,17 +653,21 @@
                         _LOGGER.debug(
                             "Marées France Coordinator: Found closest water level height: "
                             "%.2f m at %s (diff: %s)",
-                            current_water_height, closest_entry[0], min_diff
+                            current_water_height,
+                            closest_entry[0],
+                            min_diff,
                         )
                     except (ValueError, TypeError):
                         _LOGGER.warning(
                             "Marées France Coordinator: Could not parse height from "
-                            "closest water level entry: %s", closest_entry
+                            "closest water level entry: %s",
+                            closest_entry,
                         )
                 else:
                     _LOGGER.warning(
                         "Marées France Coordinator: Closest water level entry is too old "
-                        "(%s difference). Cannot determine current height.", min_diff
+                        "(%s difference). Cannot determine current height.",
+                        min_diff,
                     )
             else:  # No valid entry found in water_levels
                 _LOGGER.warning(
@@ -616,7 +694,8 @@
             if daily_coeffs and isinstance(daily_coeffs, list):
                 try:
                     valid_coeffs_int = [
-                        int(c) for c in daily_coeffs
+                        int(c)
+                        for c in daily_coeffs
                         if isinstance(c, str) and c.isdigit()
                     ]
                     if not valid_coeffs_int:
@@ -629,7 +708,9 @@
                         found_spring = True
                         _LOGGER.debug(
                             "Marées France Coordinator: Found next Spring Tide date: %s "
-                            "(Coeff: %s)", next_spring_date_str, next_spring_coeff
+                            "(Coeff: %s)",
+                            next_spring_date_str,
+                            next_spring_coeff,
                         )
                     if not found_neap and max_coeff <= NEAP_TIDE_THRESHOLD:
                         next_neap_date_str = day_str
@@ -637,18 +718,25 @@
                         found_neap = True
                         _LOGGER.debug(
                             "Marées France Coordinator: Found next Neap Tide date: %s "
-                            "(Coeff: %s)", next_neap_date_str, next_neap_coeff
+                            "(Coeff: %s)",
+                            next_neap_date_str,
+                            next_neap_coeff,
                         )
                 except (ValueError, TypeError):
                     _LOGGER.warning(
                         "Marées France Coordinator: Error processing coefficients for %s: %s",
-                        day_str, daily_coeffs
+                        day_str,
+                        daily_coeffs,
                     )
             if found_spring and found_neap:
                 break
 
-        next_spring_date_obj = date.fromisoformat(next_spring_date_str) if next_spring_date_str else None
-        next_neap_date_obj = date.fromisoformat(next_neap_date_str) if next_neap_date_str else None
+        next_spring_date_obj = (
+            date.fromisoformat(next_spring_date_str) if next_spring_date_str else None
+        )
+        next_neap_date_obj = (
+            date.fromisoformat(next_neap_date_str) if next_neap_date_str else None
+        )
 
         final_data = {
             "now_data": now_data,

--- custom_components/marees_france/sensor.py
+++ custom_components/marees_france/sensor.py
@@ -63,7 +63,9 @@
     _LOGGER.debug("Added 5 Marées France sensors for harbor: %s", harbor_id)
 
 
-class MareesFranceBaseSensor(CoordinatorEntity[MareesFranceUpdateCoordinator], SensorEntity):
+class MareesFranceBaseSensor(
+    CoordinatorEntity[MareesFranceUpdateCoordinator], SensorEntity
+):
     """Base class for Marées France sensors.
 
     Provides common attributes and functionality for all sensors derived from it,
@@ -71,7 +73,7 @@
     """
 
     _attr_attribution = ATTRIBUTION
-    _attr_has_entity_name = True # Uses the name defined by `translation_key`.
+    _attr_has_entity_name = True  # Uses the name defined by `translation_key`.
 
     def __init__(
         self,
@@ -90,17 +92,23 @@
         super().__init__(coordinator)
         self._config_entry = config_entry
         self._harbor_id: str = config_entry.data[CONF_HARBOR_ID]
-        self._harbor_name: str = config_entry.data.get(CONF_HARBOR_NAME, self._harbor_id)
-        self._sensor_key_suffix = sensor_key_suffix # Used for unique ID and data access
+        self._harbor_name: str = config_entry.data.get(
+            CONF_HARBOR_NAME, self._harbor_id
+        )
+        self._sensor_key_suffix = (
+            sensor_key_suffix  # Used for unique ID and data access
+        )
 
-        self._attr_unique_id = f"{DOMAIN}_{self._harbor_id.lower()}_{self._sensor_key_suffix}"
+        self._attr_unique_id = (
+            f"{DOMAIN}_{self._harbor_id.lower()}_{self._sensor_key_suffix}"
+        )
 
         self._attr_device_info = DeviceInfo(
             identifiers={(DOMAIN, config_entry.entry_id)},
             name=self._harbor_name,
             manufacturer=MANUFACTURER,
-            entry_type="service", # Using "service" as it's data from an external service
-            configuration_url=None, # No specific URL for device configuration
+            entry_type="service",  # Using "service" as it's data from an external service
+            configuration_url=None,  # No specific URL for device configuration
         )
         _LOGGER.debug("Initialized base sensor with unique_id: %s", self.unique_id)
 
@@ -113,7 +121,7 @@
         is present in the coordinator's data.
         """
         return (
-            super().available # Checks coordinator.last_update_success and coordinator.data
+            super().available  # Checks coordinator.last_update_success and coordinator.data
             and self.coordinator.data is not None
             # Check for the specific data key related to this sensor type
             # For "now", "next", "previous" sensors, data is under "now_data", "next_data", etc.
@@ -121,7 +129,8 @@
             and (
                 f"{self._sensor_key_suffix}_data" in self.coordinator.data
                 if self._sensor_key_suffix in ["now", "next", "previous"]
-                else self._sensor_key_suffix in self.coordinator.data # For date sensors
+                else self._sensor_key_suffix
+                in self.coordinator.data  # For date sensors
             )
         )
 
@@ -160,7 +169,9 @@
         config_entry: ConfigEntry,
     ) -> None:
         """Initialize the 'current tide' sensor."""
-        super().__init__(coordinator, config_entry, "now") # "now" is the sensor_key_suffix
+        super().__init__(
+            coordinator, config_entry, "now"
+        )  # "now" is the sensor_key_suffix
 
     @property
     def _sensor_data(self) -> dict[str, Any] | None:
@@ -180,9 +191,13 @@
         if self.available and self._sensor_data:
             attrs = {}
             for key in [
-                ATTR_TIDE_TREND, ATTR_CURRENT_HEIGHT, ATTR_COEFFICIENT,
-                ATTR_STARTING_HEIGHT, ATTR_FINISHED_HEIGHT,
-                ATTR_STARTING_TIME, ATTR_FINISHED_TIME
+                ATTR_TIDE_TREND,
+                ATTR_CURRENT_HEIGHT,
+                ATTR_COEFFICIENT,
+                ATTR_STARTING_HEIGHT,
+                ATTR_FINISHED_HEIGHT,
+                ATTR_STARTING_TIME,
+                ATTR_FINISHED_TIME,
             ]:
                 if (value := self._sensor_data.get(key)) is not None:
                     attrs[key] = value
@@ -198,7 +213,7 @@
                 return "mdi:transfer-up"
             if trend == "falling":
                 return "mdi:transfer-down"
-        return "mdi:waves" # Default icon
+        return "mdi:waves"  # Default icon
 
 
 class MareesFranceTimestampSensor(MareesFranceBaseSensor):
@@ -215,8 +230,8 @@
         self,
         coordinator: MareesFranceUpdateCoordinator,
         config_entry: ConfigEntry,
-        sensor_key_suffix: str, # e.g., "next", "previous"
-        translation_key: str,   # For entity name
+        sensor_key_suffix: str,  # e.g., "next", "previous"
+        translation_key: str,  # For entity name
     ) -> None:
         """Initialize the timestamp-based tide sensor."""
         super().__init__(coordinator, config_entry, sensor_key_suffix)
@@ -241,7 +256,8 @@
                 except ValueError:
                     _LOGGER.warning(
                         "Could not parse event time for '%s' sensor: %s",
-                        self._sensor_key_suffix, event_time_str,
+                        self._sensor_key_suffix,
+                        event_time_str,
                     )
         return None
 
@@ -249,12 +265,13 @@
     def extra_state_attributes(self) -> dict[str, Any] | None:
         """Return attributes like tide type, height, coefficient."""
         if self.available and self._sensor_data:
-            return self._sensor_data # The whole block is relevant
+            return self._sensor_data  # The whole block is relevant
         return None
 
 
 class MareesFranceNextSensor(MareesFranceTimestampSensor):
     """Sensor representing the next tide event."""
+
     def __init__(
         self,
         coordinator: MareesFranceUpdateCoordinator,
@@ -263,8 +280,10 @@
         """Initialize the 'next tide' sensor."""
         super().__init__(coordinator, config_entry, "next", "next_tide")
 
+
 class MareesFrancePreviousSensor(MareesFranceTimestampSensor):
     """Sensor representing the previous tide event."""
+
     def __init__(
         self,
         coordinator: MareesFranceUpdateCoordinator,
@@ -273,19 +292,21 @@
         """Initialize the 'previous tide' sensor."""
         super().__init__(coordinator, config_entry, "previous", "previous_tide")
 
+
 class MareesFranceNextSpecialTideSensor(MareesFranceBaseSensor):
     """Base sensor for next spring/neap tide dates.
 
     The state is the date of the next special tide (spring or neap).
     The coefficient is an attribute.
     """
-    _attr_icon = "mdi:calendar-arrow-right" # Generic icon for future date
 
+    _attr_icon = "mdi:calendar-arrow-right"  # Generic icon for future date
+
     def __init__(
         self,
         coordinator: MareesFranceUpdateCoordinator,
         config_entry: ConfigEntry,
-        sensor_key_suffix: str, # "next_spring_date" or "next_neap_date"
+        sensor_key_suffix: str,  # "next_spring_date" or "next_neap_date"
         translation_key: str,
     ) -> None:
         """Initialize the special tide date sensor."""
@@ -298,7 +319,7 @@
         """Return True if coordinator has data and the specific date exists."""
         # Overrides base availability to check for the direct key (e.g., "next_spring_date")
         return (
-            super(CoordinatorEntity, self).available # Check coordinator health
+            super(CoordinatorEntity, self).available  # Check coordinator health
             and self.coordinator.data is not None
             and self.coordinator.data.get(self._sensor_key_suffix) is not None
         )
@@ -311,7 +332,7 @@
             date_obj = self.coordinator.data.get(self._sensor_key_suffix)
             if date_obj:
                 # Handle both datetime objects and strings
-                if hasattr(date_obj, 'isoformat'):
+                if hasattr(date_obj, "isoformat"):
                     return date_obj.isoformat()
                 else:
                     # If it's already a string, just return it
@@ -324,14 +345,20 @@
         """Return the coefficient as an attribute."""
         if self.available:
             # Determine attribute key based on sensor type
-            coeff_key = "next_spring_coeff" if "spring" in self._sensor_key_suffix else "next_neap_coeff"
+            coeff_key = (
+                "next_spring_coeff"
+                if "spring" in self._sensor_key_suffix
+                else "next_neap_coeff"
+            )
             coeff = self.coordinator.data.get(coeff_key)
             if coeff is not None:
                 return {ATTR_COEFFICIENT: coeff}
         return None
 
+
 class MareesFranceNextSpringTideSensor(MareesFranceNextSpecialTideSensor):
     """Sensor for the date and coefficient of the next spring tide."""
+
     _attr_translation_key = "next_spring_tide"
     # Consider a more specific icon if desired, e.g., mdi:waves-arrow-up
 
@@ -341,10 +368,14 @@
         config_entry: ConfigEntry,
     ) -> None:
         """Initialize the next spring tide sensor."""
-        super().__init__(coordinator, config_entry, "next_spring_date", "next_spring_tide")
+        super().__init__(
+            coordinator, config_entry, "next_spring_date", "next_spring_tide"
+        )
+
 
 class MareesFranceNextNeapTideSensor(MareesFranceNextSpecialTideSensor):
     """Sensor for the date and coefficient of the next neap tide."""
+
     _attr_translation_key = "next_neap_tide"
     # Consider a more specific icon if desired, e.g., mdi:waves-arrow-down
 

--- tests/conftest.py
+++ tests/conftest.py
@@ -21,10 +21,11 @@
 # Apply the enable_socket marker to all tests in this directory and subdirectories
 pytestmark = [
     pytest.mark.enable_socket,
-    pytest.mark.enable_custom_integrations  # This is crucial for loading custom components
+    pytest.mark.enable_custom_integrations,  # This is crucial for loading custom components
 ]
 """Fixtures for Marees France integration tests."""
 
+
 # Mock modules needed for frontend dependency
 @pytest.fixture(autouse=True)
 def mock_hass_frontend():
@@ -37,34 +38,43 @@
     with patch.dict("sys.modules", modules):
         yield
 
+
 @pytest.fixture(autouse=True)
 def mock_frontend_setup():
     """Mock the frontend setup."""
     with patch("homeassistant.components.frontend.async_setup", return_value=True):
         yield
 
+
 @pytest.fixture(autouse=True)
 def mock_js_module():
     """Mock the JSModuleRegistration class."""
     mock_instance = MagicMock()
     mock_instance.async_register = AsyncMock(return_value=True)
-    
-    with patch("custom_components.marees_france.frontend.JSModuleRegistration", return_value=mock_instance):
+
+    with patch(
+        "custom_components.marees_france.frontend.JSModuleRegistration",
+        return_value=mock_instance,
+    ):
         yield
 
+
 @pytest.fixture(autouse=True)
 def mock_config_flow():
     """Mock the config flow handler."""
     # Import the config flow class
     from custom_components.marees_france.config_flow import MareesFranceConfigFlow
-    
+
     # Create a mock async_get_flow_handler function with the correct signature
     async def mock_async_get_flow_handler(hass, handler, context, data=None):
         # Return the class itself, not an instance
         return MareesFranceConfigFlow
-    
+
     # Patch the _async_get_flow_handler function
-    with patch("homeassistant.config_entries._async_get_flow_handler", side_effect=mock_async_get_flow_handler):
+    with patch(
+        "homeassistant.config_entries._async_get_flow_handler",
+        side_effect=mock_async_get_flow_handler,
+    ):
         yield
 
 
@@ -105,27 +115,36 @@
 def mock_setup_entry() -> Generator[AsyncMock, None, None]:
     """Override async_setup_entry."""
     with patch(
-        "custom_components.marees_france.async_setup_entry", return_value=AsyncMock(return_value=True)
+        "custom_components.marees_france.async_setup_entry",
+        return_value=AsyncMock(return_value=True),
     ) as mock_setup:
         yield mock_setup
 
 
 @pytest.fixture(name="mock_api_fetchers")
-def mock_api_fetchers_fixture() -> Generator[tuple[AsyncMock, AsyncMock, AsyncMock], None, None]:
+def mock_api_fetchers_fixture() -> Generator[
+    tuple[AsyncMock, AsyncMock, AsyncMock], None, None
+]:
     """Mock the API helper fetch functions used by the coordinator."""
-    with patch(
-        "custom_components.marees_france.coordinator._async_fetch_and_store_tides",
-        autospec=True,
-        return_value=True, # Assume success for tests
-    ) as mock_fetch_tides, patch(
-        "custom_components.marees_france.coordinator._async_fetch_and_store_coefficients",
-        autospec=True,
-        return_value=True, # Assume success for tests
-    ) as mock_fetch_coeffs, patch(
-        "custom_components.marees_france.coordinator._async_fetch_and_store_water_level",
-        autospec=True,
-        return_value=MOCK_PORT_DATA.get("hauteurs_maree"), # Return some plausible data
-    ) as mock_fetch_water:
+    with (
+        patch(
+            "custom_components.marees_france.coordinator._async_fetch_and_store_tides",
+            autospec=True,
+            return_value=True,  # Assume success for tests
+        ) as mock_fetch_tides,
+        patch(
+            "custom_components.marees_france.coordinator._async_fetch_and_store_coefficients",
+            autospec=True,
+            return_value=True,  # Assume success for tests
+        ) as mock_fetch_coeffs,
+        patch(
+            "custom_components.marees_france.coordinator._async_fetch_and_store_water_level",
+            autospec=True,
+            return_value=MOCK_PORT_DATA.get(
+                "hauteurs_maree"
+            ),  # Return some plausible data
+        ) as mock_fetch_water,
+    ):
         # Yield the mocks in case tests need to assert calls
         yield mock_fetch_tides, mock_fetch_coeffs, mock_fetch_water
 
@@ -140,16 +159,30 @@
         {"valeur": 90, "jour": 2, "date": "2025-05-13T00:00:00Z"},
     ],
     "hauteurs_maree": [
-        {"valeur": 1.5, "etat": "BM", "jour": 1, "heure": "03:00", "date": "2025-05-12T03:00:00Z"},
-        {"valeur": 6.5, "etat": "PM", "jour": 1, "heure": "09:00", "date": "2025-05-12T09:00:00Z"},
+        {
+            "valeur": 1.5,
+            "etat": "BM",
+            "jour": 1,
+            "heure": "03:00",
+            "date": "2025-05-12T03:00:00Z",
+        },
+        {
+            "valeur": 6.5,
+            "etat": "PM",
+            "jour": 1,
+            "heure": "09:00",
+            "date": "2025-05-12T09:00:00Z",
+        },
     ],
 }
 
+
 @pytest.fixture(autouse=True)
 def auto_enable_custom_integrations(enable_custom_integrations):
     """Enable custom integrations defined in the test dir."""
     yield
 
+
 @pytest.fixture(autouse=True)
 def mock_all_network_requests():
     """Mock all network requests to prevent hanging tests."""
@@ -160,40 +193,53 @@
     mock_response.text = AsyncMock(return_value="")
     mock_response.__aenter__.return_value = mock_response
     mock_response.__aexit__ = AsyncMock(return_value=None)
-    
+
     # Create a more aggressive set of patches
-    with patch("aiohttp.ClientSession.get", return_value=mock_response), \
-         patch("aiohttp.ClientSession.post", return_value=mock_response), \
-         patch("aiohttp.ClientSession.request", return_value=mock_response), \
-         patch("custom_components.marees_france.__init__.fetch_harbors",
-               return_value={"BREST": {"name": "Brest", "id": "BREST"}}), \
-         patch("socket.socket", side_effect=RuntimeError("Socket creation blocked")), \
-         patch("asyncio.sleep", return_value=None):  # Prevent any sleep delays
+    with (
+        patch("aiohttp.ClientSession.get", return_value=mock_response),
+        patch("aiohttp.ClientSession.post", return_value=mock_response),
+        patch("aiohttp.ClientSession.request", return_value=mock_response),
+        patch(
+            "custom_components.marees_france.__init__.fetch_harbors",
+            return_value={"BREST": {"name": "Brest", "id": "BREST"}},
+        ),
+        patch("socket.socket", side_effect=RuntimeError("Socket creation blocked")),
+        patch("asyncio.sleep", return_value=None),
+    ):  # Prevent any sleep delays
         yield
 
+
 @pytest.fixture(autouse=True)
 def expected_lingering_timers():
     """Mark that we expect lingering timers in this test module."""
     return True
 
+
 @pytest.fixture(autouse=True)
 def reduce_timeouts():
     """Reduce timeouts to speed up tests."""
     # Instead of patching constants, we'll patch the update_interval in the coordinator
     # when it's instantiated
     original_init = MareesFranceUpdateCoordinator.__init__
-    
+
     def patched_init(self, hass, entry, tides_store, coeff_store, water_level_store):
         original_init(self, hass, entry, tides_store, coeff_store, water_level_store)
         self.update_interval = timedelta(seconds=0.1)
-    
-    with patch("custom_components.marees_france.coordinator.MareesFranceUpdateCoordinator.__init__",
-               patched_init), \
-         patch("homeassistant.helpers.update_coordinator.DataUpdateCoordinator.async_refresh",
-               return_value=None), \
-         patch("asyncio.sleep", return_value=None):
+
+    with (
+        patch(
+            "custom_components.marees_france.coordinator.MareesFranceUpdateCoordinator.__init__",
+            patched_init,
+        ),
+        patch(
+            "homeassistant.helpers.update_coordinator.DataUpdateCoordinator.async_refresh",
+            return_value=None,
+        ),
+        patch("asyncio.sleep", return_value=None),
+    ):
         yield
 
+
 @pytest.fixture
 def mock_api_fetchers_detailed():
     """Mock the API fetchers with detailed data for coordinator tests."""
@@ -209,14 +255,14 @@
         "2025-05-13": [
             ["PM", "03:45", "8.0", "88"],
             ["BM", "10:00", "1.8", "---"],
-        ]
+        ],
     }
-    
+
     mock_coeffs_data = {
         "2025-05-12": ["90", "95"],
         "2025-05-13": ["88", "90"],
     }
-    
+
     mock_water_level_data = {
         "2025-05-12": [
             ["02:55:00", "8.15"],
@@ -227,7 +273,7 @@
             ["09:20:00", "1.52"],
         ]
     }
-    
+
     # Create a properly structured mock data for the coordinator
     mock_parsed_data = {
         "now_data": {
@@ -237,7 +283,7 @@
             "starting_height": "8.2",
             "finished_height": "1.5",
             "coefficient": "90",
-            "current_height": 5.0
+            "current_height": 5.0,
         },
         "next_data": {
             "tide_trend": "Low Tide",
@@ -245,7 +291,7 @@
             "finished_time": "2025-05-12T09:15:00+00:00",
             "starting_height": "8.2",
             "finished_height": "1.5",
-            "coefficient": "90"
+            "coefficient": "90",
         },
         "previous_data": {
             "tide_trend": "High Tide",
@@ -253,59 +299,83 @@
             "finished_time": "2025-05-12T03:00:00+00:00",
             "starting_height": "1.8",
             "finished_height": "8.2",
-            "coefficient": "90"
+            "coefficient": "90",
         },
         "next_spring_date": "2025-05-12",
         "next_spring_coeff": "95",
         "next_neap_date": "2025-05-13",
         "next_neap_coeff": "88",
-        "last_update": "2025-05-12T00:00:00Z"
+        "last_update": "2025-05-12T00:00:00Z",
     }
-    
+
     # Create a mock coordinator with data
-    with patch("custom_components.marees_france.api_helpers._async_fetch_and_store_tides",
-               return_value=True) as fetch_tides_mock, \
-         patch("custom_components.marees_france.api_helpers._async_fetch_and_store_coefficients",
-               return_value=True) as fetch_coeffs_mock, \
-         patch("custom_components.marees_france.api_helpers._async_fetch_and_store_water_level",
-               return_value=MOCK_PORT_DATA["hauteurs_maree"]) as fetch_water_mock, \
-         patch("custom_components.marees_france.coordinator.MareesFranceUpdateCoordinator._parse_tide_data",
-               return_value=mock_parsed_data):
-        
+    with (
+        patch(
+            "custom_components.marees_france.api_helpers._async_fetch_and_store_tides",
+            return_value=True,
+        ) as fetch_tides_mock,
+        patch(
+            "custom_components.marees_france.api_helpers._async_fetch_and_store_coefficients",
+            return_value=True,
+        ) as fetch_coeffs_mock,
+        patch(
+            "custom_components.marees_france.api_helpers._async_fetch_and_store_water_level",
+            return_value=MOCK_PORT_DATA["hauteurs_maree"],
+        ) as fetch_water_mock,
+        patch(
+            "custom_components.marees_france.coordinator.MareesFranceUpdateCoordinator._parse_tide_data",
+            return_value=mock_parsed_data,
+        ),
+    ):
         # Mock the store objects
         mock_tides_store = MagicMock()
         mock_tides_store.async_load = AsyncMock(return_value={"BREST": mock_tides_data})
         mock_tides_store.async_save = AsyncMock(return_value=None)
-        
+
         mock_coeffs_store = MagicMock()
-        mock_coeffs_store.async_load = AsyncMock(return_value={"BREST": mock_coeffs_data})
+        mock_coeffs_store.async_load = AsyncMock(
+            return_value={"BREST": mock_coeffs_data}
+        )
         mock_coeffs_store.async_save = AsyncMock(return_value=None)
-        
+
         mock_water_store = MagicMock()
-        mock_water_store.async_load = AsyncMock(return_value={"BREST": mock_water_level_data})
+        mock_water_store.async_load = AsyncMock(
+            return_value={"BREST": mock_water_level_data}
+        )
         mock_water_store.async_save = AsyncMock(return_value=None)
-        
+
         # Patch the store creation
-        with patch("custom_components.marees_france.__init__.Store", side_effect=[
-                mock_tides_store, mock_coeffs_store, mock_water_store
-            ]):
+        with patch(
+            "custom_components.marees_france.__init__.Store",
+            side_effect=[mock_tides_store, mock_coeffs_store, mock_water_store],
+        ):
             yield (fetch_tides_mock, fetch_coeffs_mock, fetch_water_mock)
 
+
 @pytest.fixture
-async def init_integration(
-    hass: HomeAssistant, mock_api_fetchers_detailed
-) -> None:
+async def init_integration(hass: HomeAssistant, mock_api_fetchers_detailed) -> None:
     """Set up the Marees France integration for testing."""
     # Mock the frontend module to prevent lingering timers
-    with patch("custom_components.marees_france.__init__.JSModuleRegistration") as mock_js, \
-         patch("custom_components.marees_france.frontend.JSModuleRegistration._async_wait_for_lovelace_resources", return_value=None), \
-         patch("homeassistant.helpers.event.async_call_later", return_value=lambda: None), \
-         patch("custom_components.marees_france.__init__.fetch_harbors", return_value={"BREST": {"name": "Brest", "id": "BREST"}}):
-        
+    with (
+        patch(
+            "custom_components.marees_france.__init__.JSModuleRegistration"
+        ) as mock_js,
+        patch(
+            "custom_components.marees_france.frontend.JSModuleRegistration._async_wait_for_lovelace_resources",
+            return_value=None,
+        ),
+        patch(
+            "homeassistant.helpers.event.async_call_later", return_value=lambda: None
+        ),
+        patch(
+            "custom_components.marees_france.__init__.fetch_harbors",
+            return_value={"BREST": {"name": "Brest", "id": "BREST"}},
+        ),
+    ):
         mock_instance = MagicMock()
         mock_instance.async_register = MagicMock(return_value=True)
         mock_js.return_value = mock_instance
-        
+
         # Create a mock config entry
         entry = MockConfigEntry(
             domain=DOMAIN,
@@ -314,11 +384,11 @@
             unique_id="BREST",
         )
         entry.add_to_hass(hass)
-        
+
         # We need to remove the entry and add it again to avoid migration errors
         await hass.config_entries.async_remove(entry.entry_id)
         await hass.async_block_till_done()
-        
+
         # Create a new entry
         entry = MockConfigEntry(
             domain=DOMAIN,
@@ -328,11 +398,11 @@
             version=2,  # Skip migration
         )
         entry.add_to_hass(hass)
-        
+
         # Set up the component first
         assert await async_setup_component(hass, DOMAIN, {})
         await hass.async_block_till_done()
-        
+
         # Then set up the entry
         assert await hass.config_entries.async_setup(entry.entry_id)
-        await hass.async_block_till_done()
\ No newline at end of file
+        await hass.async_block_till_done()

--- tests/test_config_flow.py
+++ tests/test_config_flow.py
@@ -1,4 +1,5 @@
 """Test the Marees France config flow."""
+
 from unittest.mock import AsyncMock, patch
 
 import pytest
@@ -10,20 +11,26 @@
 from pytest_homeassistant_custom_component.common import MockConfigEntry
 
 from custom_components.marees_france.const import (
-   CONF_HARBOR_ID,
+    CONF_HARBOR_ID,
     CONF_HARBOR_NAME,
     DOMAIN,
     INTEGRATION_NAME,
 )
-from custom_components.marees_france.config_flow import CannotConnect, MareesFranceConfigFlow
+from custom_components.marees_france.config_flow import (
+    CannotConnect,
+    MareesFranceConfigFlow,
+)
 
 from tests.conftest import MOCK_CONFIG_ENTRY_DATA, MOCK_PORT_DATA
 
 # Define a mock harbor cache based on MOCK_PORT_DATA
 MOCK_HARBOR_ID = MOCK_CONFIG_ENTRY_DATA[CONF_HARBOR_ID]
-MOCK_HARBOR_NAME = MOCK_PORT_DATA["nom_port"] # Use the name from MOCK_PORT_DATA
+MOCK_HARBOR_NAME = MOCK_PORT_DATA["nom_port"]  # Use the name from MOCK_PORT_DATA
 MOCK_HARBORS_CACHE = {
-    MOCK_HARBOR_ID: {"name": MOCK_HARBOR_NAME, "display": f"{MOCK_HARBOR_NAME} ({MOCK_HARBOR_ID})"},
+    MOCK_HARBOR_ID: {
+        "name": MOCK_HARBOR_NAME,
+        "display": f"{MOCK_HARBOR_NAME} ({MOCK_HARBOR_ID})",
+    },
     "OTHER_ID": {"name": "Other Port", "display": "Other Port (OTHER_ID)"},
 }
 
@@ -33,12 +40,13 @@
     """Mark that we expect lingering timers in this test module."""
     return True
 
+
 @pytest.fixture(name="mock_fetch_harbors")
 def fixture_mock_fetch_harbors():
     """Mock the fetch_harbors function."""
     # Create a modified cache that includes the invalid harbor ID for testing
     test_harbors_cache = MOCK_HARBORS_CACHE.copy()
-    
+
     with patch(
         "custom_components.marees_france.config_flow.fetch_harbors",
         return_value=test_harbors_cache,
@@ -54,7 +62,7 @@
     result = await hass.config_entries.flow.async_init(
         DOMAIN, context={"source": SOURCE_USER}
     )
-    await hass.async_block_till_done() # Allow fetch_harbors mock to be called
+    await hass.async_block_till_done()  # Allow fetch_harbors mock to be called
 
     # Check that the form is shown
     assert result is not None
@@ -62,7 +70,7 @@
     assert result["step_id"] == "user"
     # In newer Home Assistant versions, errors is an empty dict instead of None
     assert result["errors"] == {}
-    mock_fetch_harbors.assert_called_once() # Check fetch_harbors was called
+    mock_fetch_harbors.assert_called_once()  # Check fetch_harbors was called
 
     # Simulate user input selecting the mock harbor
     result2 = await hass.config_entries.flow.async_configure(
@@ -78,7 +86,7 @@
     assert result2["title"] == f"{INTEGRATION_NAME} - {MOCK_HARBOR_NAME}"
     assert result2["data"] == {
         CONF_HARBOR_ID: MOCK_HARBOR_ID,
-        CONF_HARBOR_NAME: MOCK_HARBOR_NAME, # Ensure name is also stored
+        CONF_HARBOR_NAME: MOCK_HARBOR_NAME,  # Ensure name is also stored
     }
     # No second call to fetch_harbors expected here
     mock_fetch_harbors.assert_called_once()
@@ -91,13 +99,13 @@
     # Create a direct instance of the config flow
     flow = MareesFranceConfigFlow()
     flow.hass = hass
-    
+
     # Mock the _harbors_cache with a valid harbor
     flow._harbors_cache = {"BREST": {"name": "Brest", "display": "Brest (BREST)"}}
-    
+
     # Test with an invalid harbor ID
     result = await flow.async_step_user({"harbor_id": "INVALID_HARBOR_ID"})
-    
+
     # Check that the form is shown again with an error
     assert result["type"] == FlowResultType.FORM
     assert result["step_id"] == "user"
@@ -149,7 +157,7 @@
     # Setup an existing entry using the mock data
     existing_entry = MockConfigEntry(
         domain=DOMAIN,
-        unique_id=MOCK_HARBOR_ID.lower(), # unique_id is lowercase harbor_id
+        unique_id=MOCK_HARBOR_ID.lower(),  # unique_id is lowercase harbor_id
         data={CONF_HARBOR_ID: MOCK_HARBOR_ID, CONF_HARBOR_NAME: MOCK_HARBOR_NAME},
         title=f"{INTEGRATION_NAME} - {MOCK_HARBOR_NAME}",
     )
@@ -249,4 +257,4 @@
 # assert result2["type"] == FlowResultType.FORM
 # assert result2["step_id"] == "init" # Or your specific options step_id
 # assert result2["errors"] is not None # Check for specific errors
-#     # e.g., assert result2["errors"]["base"] == "invalid_option_value"
\ No newline at end of file
+#     # e.g., assert result2["errors"]["base"] == "invalid_option_value"

--- tests/test_coordinator.py
+++ tests/test_coordinator.py
@@ -15,44 +15,47 @@
 # Assuming ShomApiClient and specific exceptions are importable for mocking/testing
 # from custom_components.marees_france.api import ShomApiClient, ShomApiError
 
-from tests.conftest import MOCK_CONFIG_ENTRY_DATA, CONF_HARBOR_ID # MOCK_PORT_DATA is likely obsolete now
+from tests.conftest import (
+    MOCK_CONFIG_ENTRY_DATA,
+    CONF_HARBOR_ID,
+)  # MOCK_PORT_DATA is likely obsolete now
 
 
 # Mock data simulating the structure stored in the cache by helper functions
 # Dates are chosen around the test's reference 'now' (2025-05-12T00:00:00Z)
 MOCK_TIDES_CACHE = {
     MOCK_CONFIG_ENTRY_DATA[CONF_HARBOR_ID]: {
-        "2025-05-11": [ # Yesterday
+        "2025-05-11": [  # Yesterday
             ["BM", "20:45", "1.8", "---"],
         ],
-        "2025-05-12": [ # Today
+        "2025-05-12": [  # Today
             ["PM", "03:00", "8.2", "90"],
             ["BM", "09:15", "1.5", "---"],
             ["PM", "15:30", "8.5", "95"],
             ["BM", "21:45", "1.2", "---"],
         ],
-        "2025-05-13": [ # Tomorrow
+        "2025-05-13": [  # Tomorrow
             ["PM", "03:45", "8.0", "88"],
             ["BM", "10:00", "1.8", "---"],
-        ]
+        ],
         # ... potentially more days
     }
 }
 MOCK_COEFF_CACHE = {
     MOCK_CONFIG_ENTRY_DATA[CONF_HARBOR_ID]: {
-        "2025-05-12": ["90", "95"], # Today
-        "2025-05-13": ["88", "85"], # Tomorrow
+        "2025-05-12": ["90", "95"],  # Today
+        "2025-05-13": ["88", "85"],  # Tomorrow
         # ... potentially more days
     }
 }
 MOCK_WATER_LEVEL_CACHE = {
     MOCK_CONFIG_ENTRY_DATA[CONF_HARBOR_ID]: {
-        "2025-05-12": [ # Today
+        "2025-05-12": [  # Today
             ["02:55:00", "8.15"],
-            ["03:00:00", "8.20"], # Matches high tide time
+            ["03:00:00", "8.20"],  # Matches high tide time
             ["03:05:00", "8.18"],
             ["09:10:00", "1.55"],
-            ["09:15:00", "1.50"], # Matches low tide time
+            ["09:15:00", "1.50"],  # Matches low tide time
             ["09:20:00", "1.52"],
             # ... more readings for the day
         ]
@@ -80,7 +83,9 @@
 async def setup_coordinator(
     hass: HomeAssistant,
     mock_stores: tuple[AsyncMock, AsyncMock, AsyncMock],
-) -> tuple[MareesFranceUpdateCoordinator, AsyncMock, AsyncMock, AsyncMock, MockConfigEntry]:
+) -> tuple[
+    MareesFranceUpdateCoordinator, AsyncMock, AsyncMock, AsyncMock, MockConfigEntry
+]:
     """Set up the MareesFranceUpdateCoordinator with mock stores."""
     mock_tides_store, mock_coeff_store, mock_water_level_store = mock_stores
     entry = MockConfigEntry(
@@ -105,7 +110,7 @@
             "starting_height": "8.2",
             "finished_height": "1.5",
             "coefficient": "90",
-            "current_height": 5.0
+            "current_height": 5.0,
         },
         "next_data": {
             "tide_trend": "Low Tide",
@@ -113,7 +118,7 @@
             "finished_time": "2025-05-12T09:15:00+00:00",
             "starting_height": "8.2",
             "finished_height": "1.5",
-            "coefficient": "90"
+            "coefficient": "90",
         },
         "previous_data": {
             "tide_trend": "High Tide",
@@ -121,15 +126,15 @@
             "finished_time": "2025-05-12T03:00:00+00:00",
             "starting_height": "1.8",
             "finished_height": "8.2",
-            "coefficient": "90"
+            "coefficient": "90",
         },
         "next_spring_date": "2025-05-12",
         "next_spring_coeff": "95",
         "next_neap_date": "2025-05-13",
         "next_neap_coeff": "88",
-        "last_update": "2025-05-12T00:00:00Z"
+        "last_update": "2025-05-12T00:00:00Z",
     }
-    
+
     coordinator = MareesFranceUpdateCoordinator(
         hass, entry, mock_tides_store, mock_coeff_store, mock_water_level_store
     )
@@ -137,18 +142,26 @@
     # Prevent coordinator's scheduled updates during tests by default
     # Tests can manually trigger updates using async_refresh()
     coordinator.update_interval = None
-    
+
     # Directly set the data property
     coordinator.data = mock_data
     coordinator.last_update_success = True
-    
-    return coordinator, mock_tides_store, mock_coeff_store, mock_water_level_store, entry
 
+    return (
+        coordinator,
+        mock_tides_store,
+        mock_coeff_store,
+        mock_water_level_store,
+        entry,
+    )
+
 
 async def test_coordinator_initial_fetch_success(
     hass: HomeAssistant,
-    setup_coordinator: tuple[MareesFranceUpdateCoordinator, AsyncMock, AsyncMock, AsyncMock, MockConfigEntry],
-    mock_api_fetchers_detailed: MagicMock, # Access patched helpers if needed
+    setup_coordinator: tuple[
+        MareesFranceUpdateCoordinator, AsyncMock, AsyncMock, AsyncMock, MockConfigEntry
+    ],
+    mock_api_fetchers_detailed: MagicMock,  # Access patched helpers if needed
     snapshot: SnapshotAssertion,
 ):
     """Test successful initial data fetch by the coordinator."""
@@ -158,15 +171,17 @@
     # we just need to verify it's correct
     assert coordinator.last_update_success is True
     assert coordinator.data is not None
-    
+
     # Skip snapshot testing for now
     # assert coordinator.data == snapshot
 
 
 async def test_coordinator_listener_updated_on_success(
     hass: HomeAssistant,
-    setup_coordinator: tuple[MareesFranceUpdateCoordinator, AsyncMock, AsyncMock, AsyncMock, MockConfigEntry],
-    mock_api_fetchers_detailed: MagicMock, # Access patched helpers if needed
+    setup_coordinator: tuple[
+        MareesFranceUpdateCoordinator, AsyncMock, AsyncMock, AsyncMock, MockConfigEntry
+    ],
+    mock_api_fetchers_detailed: MagicMock,  # Access patched helpers if needed
 ):
     """Test that listeners are updated after a successful data fetch."""
     coordinator, mock_tides, mock_coeffs, mock_water, _ = setup_coordinator
@@ -174,53 +189,57 @@
     # Add a regular function as a listener instead of an AsyncMock
     # to avoid the "coroutine never awaited" warning
     called = False
-    
+
     def listener_callback():
         nonlocal called
         called = True
-    
+
     coordinator.async_add_listener(listener_callback)
 
     # Manually trigger the listeners
     coordinator.async_update_listeners()
-    
+
     # Verify the listener was called
     assert called is True
 
 
 async def test_coordinator_api_error_handling(
     hass: HomeAssistant,
-    setup_coordinator: tuple[MareesFranceUpdateCoordinator, AsyncMock, AsyncMock, AsyncMock, MockConfigEntry],
-    mock_api_fetchers_detailed: MagicMock, # Access patched helpers
+    setup_coordinator: tuple[
+        MareesFranceUpdateCoordinator, AsyncMock, AsyncMock, AsyncMock, MockConfigEntry
+    ],
+    mock_api_fetchers_detailed: MagicMock,  # Access patched helpers
 ):
     """Test coordinator error handling when the API call fails during cache repair."""
     coordinator, mock_tides, mock_coeffs, mock_water, _ = setup_coordinator
 
     # Simulate an error by directly setting the coordinator state
     coordinator.last_update_success = False
-    
+
     # Verify the coordinator state
     assert coordinator.last_update_success is False
 
 
 async def test_coordinator_recovery_after_api_error(
     hass: HomeAssistant,
-    setup_coordinator: tuple[MareesFranceUpdateCoordinator, AsyncMock, AsyncMock, AsyncMock, MockConfigEntry],
-    mock_api_fetchers_detailed: MagicMock, # Access patched helpers
+    setup_coordinator: tuple[
+        MareesFranceUpdateCoordinator, AsyncMock, AsyncMock, AsyncMock, MockConfigEntry
+    ],
+    mock_api_fetchers_detailed: MagicMock,  # Access patched helpers
     snapshot: SnapshotAssertion,
 ):
     """Test coordinator recovers and fetches data after a previous API error."""
     coordinator, mock_tides, mock_coeffs, mock_water, _ = setup_coordinator
-    
+
     # First, simulate an error
     coordinator.last_update_success = False
-    
+
     # Verify the coordinator state after error
     assert coordinator.last_update_success is False
-    
+
     # Then, simulate recovery
     coordinator.last_update_success = True
-    
+
     # Verify the coordinator state after recovery
     assert coordinator.last_update_success is True
     assert coordinator.data is not None
@@ -228,19 +247,21 @@
 
 async def test_coordinator_scheduled_update(
     hass: HomeAssistant,
-    setup_coordinator: tuple[MareesFranceUpdateCoordinator, AsyncMock, AsyncMock, AsyncMock, MockConfigEntry],
-    mock_api_fetchers_detailed: MagicMock, # Access patched helpers
-    freezer, # Use time freezing fixture
+    setup_coordinator: tuple[
+        MareesFranceUpdateCoordinator, AsyncMock, AsyncMock, AsyncMock, MockConfigEntry
+    ],
+    mock_api_fetchers_detailed: MagicMock,  # Access patched helpers
+    freezer,  # Use time freezing fixture
 ):
     """Test scheduled updates trigger data fetching."""
     coordinator, mock_tides, mock_coeffs, mock_water, entry = setup_coordinator
 
     # Set the update interval
     coordinator.update_interval = timedelta(minutes=5)
-    
+
     # Verify the coordinator has the correct update interval
     assert coordinator.update_interval == timedelta(minutes=5)
-    
+
     # Verify the coordinator state
     assert coordinator.last_update_success is True
 
@@ -250,4 +271,4 @@
 # The mock cache data and test assertions might need further refinement based
 # on the exact logic within _async_update_data and _parse_tide_data.
 # mock_api_fetchers is a tuple of three AsyncMock objects:
-# (mock_fetch_tides, mock_fetch_coeffs, mock_fetch_water)
\ No newline at end of file
+# (mock_fetch_tides, mock_fetch_coeffs, mock_fetch_water)

--- tests/test_import.py
+++ tests/test_import.py
@@ -1,27 +1,34 @@
 """Test importing the integration."""
 
+
 def test_import():
     """Test that the integration can be imported."""
     import custom_components.marees_france
+
     assert custom_components.marees_france is not None
-    
+
     # Try importing specific modules
     from custom_components.marees_france import const
+
     assert const.DOMAIN == "marees_france"
-    
+
     from custom_components.marees_france import config_flow
+
     assert config_flow.MareesFranceConfigFlow is not None
-    
+
     from custom_components.marees_france import coordinator
+
     assert coordinator.MareesFranceUpdateCoordinator is not None
-    
+
     from custom_components.marees_france import sensor
+
     assert sensor is not None
-    
+
     # Try importing the frontend module
     try:
         from custom_components.marees_france import frontend
+
         assert frontend is not None
         print("Frontend module imported successfully")
     except ImportError as e:
-        print(f"Error importing frontend module: {e}")
\ No newline at end of file
+        print(f"Error importing frontend module: {e}")

--- tests/test_init.py
+++ tests/test_init.py
@@ -1,4 +1,5 @@
 """Tests for the Marees France integration."""
+
 from unittest.mock import patch
 
 import pytest
@@ -10,6 +11,7 @@
 from custom_components.marees_france.const import DOMAIN
 from tests.conftest import MOCK_CONFIG_ENTRY_DATA
 
+
 @pytest.fixture(autouse=True)
 def expected_lingering_timers():
     """Mark that we expect lingering timers in this test module."""
@@ -29,23 +31,35 @@
     entry.add_to_hass(hass)
 
     # Mock all the necessary functions
-    with patch("custom_components.marees_france.coordinator.MareesFranceUpdateCoordinator.async_refresh", return_value=None), \
-         patch("custom_components.marees_france.frontend.JSModuleRegistration._async_wait_for_lovelace_resources", return_value=None), \
-         patch("homeassistant.helpers.event.async_call_later", return_value=lambda: None), \
-         patch("custom_components.marees_france.__init__.fetch_harbors", return_value={"BREST": {"name": "Brest", "id": "BREST"}}), \
-         patch("custom_components.marees_france.async_setup", return_value=True):
-        
+    with (
+        patch(
+            "custom_components.marees_france.coordinator.MareesFranceUpdateCoordinator.async_refresh",
+            return_value=None,
+        ),
+        patch(
+            "custom_components.marees_france.frontend.JSModuleRegistration._async_wait_for_lovelace_resources",
+            return_value=None,
+        ),
+        patch(
+            "homeassistant.helpers.event.async_call_later", return_value=lambda: None
+        ),
+        patch(
+            "custom_components.marees_france.__init__.fetch_harbors",
+            return_value={"BREST": {"name": "Brest", "id": "BREST"}},
+        ),
+        patch("custom_components.marees_france.async_setup", return_value=True),
+    ):
         # Set up the component
         assert await async_setup_component(hass, DOMAIN, {})
         await hass.async_block_till_done()
-        
+
         # Import the setup entry function
         from custom_components.marees_france import async_setup_entry
-        
+
         # Set up the entry
         assert await async_setup_entry(hass, entry)
         await hass.async_block_till_done()
-        
+
         # Basic assertions
         assert DOMAIN in hass.data
         assert entry.entry_id in hass.data[DOMAIN]
@@ -64,30 +78,45 @@
     entry.add_to_hass(hass)
 
     # Mock all the necessary functions
-    with patch("custom_components.marees_france.coordinator.MareesFranceUpdateCoordinator.async_refresh", return_value=None), \
-         patch("custom_components.marees_france.frontend.JSModuleRegistration._async_wait_for_lovelace_resources", return_value=None), \
-         patch("homeassistant.helpers.event.async_call_later", return_value=lambda: None), \
-         patch("custom_components.marees_france.__init__.fetch_harbors", return_value={"BREST": {"name": "Brest", "id": "BREST"}}), \
-         patch("custom_components.marees_france.async_setup", return_value=True):
-        
+    with (
+        patch(
+            "custom_components.marees_france.coordinator.MareesFranceUpdateCoordinator.async_refresh",
+            return_value=None,
+        ),
+        patch(
+            "custom_components.marees_france.frontend.JSModuleRegistration._async_wait_for_lovelace_resources",
+            return_value=None,
+        ),
+        patch(
+            "homeassistant.helpers.event.async_call_later", return_value=lambda: None
+        ),
+        patch(
+            "custom_components.marees_france.__init__.fetch_harbors",
+            return_value={"BREST": {"name": "Brest", "id": "BREST"}},
+        ),
+        patch("custom_components.marees_france.async_setup", return_value=True),
+    ):
         # Set up the component
         assert await async_setup_component(hass, DOMAIN, {})
         await hass.async_block_till_done()
-        
+
         # Import the setup and unload entry functions
-        from custom_components.marees_france import async_setup_entry, async_unload_entry
-        
+        from custom_components.marees_france import (
+            async_setup_entry,
+            async_unload_entry,
+        )
+
         # Set up the entry
         assert await async_setup_entry(hass, entry)
         await hass.async_block_till_done()
-        
+
         # Basic assertions before unloading
         assert DOMAIN in hass.data
         assert entry.entry_id in hass.data[DOMAIN]
-        
+
         # Unload the entry
         assert await async_unload_entry(hass, entry)
         await hass.async_block_till_done()
-        
+
         # Check that the entry was unloaded
-        assert entry.entry_id not in hass.data.get(DOMAIN, {})
\ No newline at end of file
+        assert entry.entry_id not in hass.data.get(DOMAIN, {})

--- tests/test_mock_dependencies.py
+++ tests/test_mock_dependencies.py
@@ -1,4 +1,5 @@
 """Test with mocked dependencies."""
+
 from unittest.mock import patch, MagicMock
 
 import pytest
@@ -31,8 +32,11 @@
     """Mock the JSModuleRegistration class."""
     mock_instance = MagicMock()
     mock_instance.async_register = MagicMock(return_value=True)
-    
-    with patch("custom_components.marees_france.frontend.JSModuleRegistration", return_value=mock_instance):
+
+    with patch(
+        "custom_components.marees_france.frontend.JSModuleRegistration",
+        return_value=mock_instance,
+    ):
         yield
 
 
@@ -43,6 +47,6 @@
         # Try to set up the component
         result = await async_setup_component(hass, DOMAIN, {})
         await hass.async_block_till_done()
-        
+
         # Check if the setup was successful
-        assert result is True, "Failed to set up the component with mocked dependencies"
\ No newline at end of file
+        assert result is True, "Failed to set up the component with mocked dependencies"

--- tests/test_sensor.py
+++ tests/test_sensor.py
@@ -1,4 +1,5 @@
 """Tests for the Marees France sensor platform."""
+
 from unittest.mock import AsyncMock, patch
 
 import pytest
@@ -46,17 +47,24 @@
     KEY_FIRST_HIGH_TIDE_HEIGHT_TODAY,
 ]
 
+
 def get_entity_id(friendly_name_slug: str, sensor_key: str) -> str:
     """Helper to create entity IDs based on slugified friendly name."""
     return f"sensor.{friendly_name_slug}_{sensor_key}"
 
+
 @pytest.fixture(autouse=True)
 def expected_lingering_timers():
     """Mark that we expect lingering timers in this test module."""
     return True
 
+
 @pytest.fixture
-async def setup_integration_entry(hass: HomeAssistant, mock_api_fetchers_detailed: AsyncMock, entity_registry: er.EntityRegistry):
+async def setup_integration_entry(
+    hass: HomeAssistant,
+    mock_api_fetchers_detailed: AsyncMock,
+    entity_registry: er.EntityRegistry,
+):
     """Set up the Marees France integration with a config entry."""
     # First, check if there's already an entry with the same ID
     existing_entries = hass.config_entries.async_entries(DOMAIN)
@@ -69,7 +77,7 @@
     entry = MockConfigEntry(
         domain=DOMAIN,
         data=MOCK_CONFIG_ENTRY_DATA,
-        unique_id=MOCK_CONFIG_ENTRY_DATA[CONF_HARBOR_ID], # "BREST"
+        unique_id=MOCK_CONFIG_ENTRY_DATA[CONF_HARBOR_ID],  # "BREST"
         entry_id="test",
         version=2,  # Skip migration
     )
@@ -88,7 +96,7 @@
             "starting_height": "8.2",
             "finished_height": "1.5",
             "coefficient": "90",
-            "current_height": 5.0
+            "current_height": 5.0,
         },
         "next_data": {
             "tide_trend": "Low Tide",
@@ -96,7 +104,7 @@
             "finished_time": "2025-05-12T09:15:00+00:00",
             "starting_height": "8.2",
             "finished_height": "1.5",
-            "coefficient": "90"
+            "coefficient": "90",
         },
         "previous_data": {
             "tide_trend": "High Tide",
@@ -104,20 +112,23 @@
             "finished_time": "2025-05-12T03:00:00+00:00",
             "starting_height": "1.8",
             "finished_height": "8.2",
-            "coefficient": "90"
+            "coefficient": "90",
         },
         "next_spring_date": "2025-05-12",
         "next_spring_coeff": "95",
         "next_neap_date": "2025-05-13",
         "next_neap_coeff": "88",
-        "last_update": "2025-05-12T00:00:00+00:00"
+        "last_update": "2025-05-12T00:00:00+00:00",
     }
 
     # Patch the coordinator's _parse_tide_data method and dt_util.now
-    with patch("custom_components.marees_france.coordinator.MareesFranceUpdateCoordinator._parse_tide_data",
-               return_value=mock_data), \
-         patch("homeassistant.util.dt.now", return_value=now):
-        
+    with (
+        patch(
+            "custom_components.marees_france.coordinator.MareesFranceUpdateCoordinator._parse_tide_data",
+            return_value=mock_data,
+        ),
+        patch("homeassistant.util.dt.now", return_value=now),
+    ):
         # Set up the entry if it's not already set up
         if entry.state != ConfigEntryState.LOADED:
             assert await hass.config_entries.async_setup(entry.entry_id)
@@ -125,18 +136,19 @@
 
     return entry
 
+
 async def test_sensor_creation_and_initial_state(
     hass: HomeAssistant,
     setup_integration_entry: MockConfigEntry,
     entity_registry: er.EntityRegistry,
-    device_registry: dr.DeviceRegistry, # Added device_registry fixture
+    device_registry: dr.DeviceRegistry,  # Added device_registry fixture
 ):
     """Test sensor entities are created and have correct initial state."""
     config_entry = setup_integration_entry
-    
+
     # Get the coordinator from hass data
     coordinator = hass.data[DOMAIN][config_entry.entry_id]
-    
+
     # Directly set the coordinator data to ensure sensors have data
     mock_data = {
         "now_data": {
@@ -146,7 +158,7 @@
             "starting_height": "8.2",
             "finished_height": "1.5",
             "coefficient": "90",
-            "current_height": 5.0
+            "current_height": 5.0,
         },
         "next_data": {
             "tide_trend": "Low Tide",
@@ -154,7 +166,7 @@
             "finished_time": "2025-05-12T09:15:00+00:00",
             "starting_height": "8.2",
             "finished_height": "1.5",
-            "coefficient": "90"
+            "coefficient": "90",
         },
         "previous_data": {
             "tide_trend": "High Tide",
@@ -162,29 +174,31 @@
             "finished_time": "2025-05-12T03:00:00+00:00",
             "starting_height": "1.8",
             "finished_height": "8.2",
-            "coefficient": "90"
+            "coefficient": "90",
         },
         "next_spring_date": "2025-05-12",
         "next_spring_coeff": "95",
         "next_neap_date": "2025-05-13",
         "next_neap_coeff": "88",
-        "last_update": "2025-05-12T00:00:00+00:00"
+        "last_update": "2025-05-12T00:00:00+00:00",
     }
     coordinator.data = mock_data
     coordinator.last_update_success = True
     coordinator.async_update_listeners()
-    
+
     # Wait for the sensors to update
     await hass.async_block_till_done()
-    
+
     # Check that the entity registry has the entities
     entities = er.async_entries_for_config_entry(entity_registry, config_entry.entry_id)
     assert len(entities) > 0, "No entities found for config entry"
-    
+
     # Check that the device registry has the device
-    device_entries = dr.async_entries_for_config_entry(device_registry, config_entry.entry_id)
+    device_entries = dr.async_entries_for_config_entry(
+        device_registry, config_entry.entry_id
+    )
     assert len(device_entries) > 0, "No devices found for config entry"
-    
+
     # Check that the sensors are available
     for entity in entities:
         state = hass.states.get(entity.entity_id)
@@ -200,7 +214,7 @@
     """Test sensor states update when coordinator provides new data."""
     config_entry = setup_integration_entry
     coordinator = hass.data[DOMAIN][config_entry.entry_id]
-    
+
     # First update with initial data
     mock_data = {
         "now_data": {
@@ -210,7 +224,7 @@
             "starting_height": "8.2",
             "finished_height": "1.5",
             "coefficient": "90",
-            "current_height": 5.0
+            "current_height": 5.0,
         },
         "next_data": {
             "tide_trend": "Low Tide",
@@ -218,7 +232,7 @@
             "finished_time": "2025-05-12T09:15:00+00:00",
             "starting_height": "8.2",
             "finished_height": "1.5",
-            "coefficient": "90"
+            "coefficient": "90",
         },
         "previous_data": {
             "tide_trend": "High Tide",
@@ -226,19 +240,19 @@
             "finished_time": "2025-05-12T03:00:00+00:00",
             "starting_height": "1.8",
             "finished_height": "8.2",
-            "coefficient": "90"
+            "coefficient": "90",
         },
         "next_spring_date": "2025-05-12",
         "next_spring_coeff": "95",
         "next_neap_date": "2025-05-13",
         "next_neap_coeff": "88",
-        "last_update": "2025-05-12T00:00:00+00:00"
+        "last_update": "2025-05-12T00:00:00+00:00",
     }
     coordinator.data = mock_data
     coordinator.last_update_success = True
     coordinator.async_update_listeners()
     await hass.async_block_till_done()
-    
+
     # Then update with new data
     new_mock_data = {
         "now_data": {
@@ -248,7 +262,7 @@
             "starting_height": "7.5",
             "finished_height": "2.0",
             "coefficient": "80",
-            "current_height": 6.0
+            "current_height": 6.0,
         },
         "next_data": {
             "tide_trend": "Low Tide",
@@ -256,7 +270,7 @@
             "finished_time": "2025-05-12T04:00:00+00:00",
             "starting_height": "7.5",
             "finished_height": "2.0",
-            "coefficient": "80"
+            "coefficient": "80",
         },
         "previous_data": {
             "tide_trend": "High Tide",
@@ -264,19 +278,19 @@
             "finished_time": "2025-05-12T04:00:00+00:00",
             "starting_height": "2.0",
             "finished_height": "7.5",
-            "coefficient": "80"
+            "coefficient": "80",
         },
         "next_spring_date": "2025-05-12",
         "next_spring_coeff": "80",
         "next_neap_date": "2025-05-13",
         "next_neap_coeff": "75",
-        "last_update": "2025-05-12T01:00:00+00:00"
+        "last_update": "2025-05-12T01:00:00+00:00",
     }
     coordinator.data = new_mock_data
     coordinator.last_update_success = True
     coordinator.async_update_listeners()
     await hass.async_block_till_done()
-    
+
     # Check that the entities exist
     entities = er.async_entries_for_config_entry(entity_registry, config_entry.entry_id)
     assert len(entities) > 0, "No entities found for config entry"
@@ -290,7 +304,7 @@
     """Test sensor availability when coordinator fails and recovers."""
     config_entry = setup_integration_entry
     coordinator = hass.data[DOMAIN][config_entry.entry_id]
-    
+
     # First, set up with good data
     mock_data = {
         "now_data": {
@@ -300,7 +314,7 @@
             "starting_height": "8.2",
             "finished_height": "1.5",
             "coefficient": "90",
-            "current_height": 5.0
+            "current_height": 5.0,
         },
         "next_data": {
             "tide_trend": "Low Tide",
@@ -308,7 +322,7 @@
             "finished_time": "2025-05-12T09:15:00+00:00",
             "starting_height": "8.2",
             "finished_height": "1.5",
-            "coefficient": "90"
+            "coefficient": "90",
         },
         "previous_data": {
             "tide_trend": "High Tide",
@@ -316,29 +330,29 @@
             "finished_time": "2025-05-12T03:00:00+00:00",
             "starting_height": "1.8",
             "finished_height": "8.2",
-            "coefficient": "90"
+            "coefficient": "90",
         },
         "next_spring_date": "2025-05-12",
         "next_spring_coeff": "95",
         "next_neap_date": "2025-05-13",
         "next_neap_coeff": "88",
-        "last_update": "2025-05-12T00:00:00+00:00"
+        "last_update": "2025-05-12T00:00:00+00:00",
     }
     coordinator.data = mock_data
     coordinator.last_update_success = True
     coordinator.async_update_listeners()
     await hass.async_block_till_done()
-    
+
     # Check that the entities exist
     entities = er.async_entries_for_config_entry(entity_registry, config_entry.entry_id)
     assert len(entities) > 0, "No entities found for config entry"
-    
+
     # Then simulate a failure
     coordinator.last_update_success = False
     coordinator.async_update_listeners()
     await hass.async_block_till_done()
-    
+
     # Then simulate recovery
     coordinator.last_update_success = True
     coordinator.async_update_listeners()
-    await hass.async_block_till_done()
\ No newline at end of file
+    await hass.async_block_till_done()

--- tests/test_services.py
+++ tests/test_services.py
@@ -18,7 +18,9 @@
 
 
 @pytest.fixture
-async def setup_integration_with_services(hass: HomeAssistant, mock_api_fetchers: AsyncMock):
+async def setup_integration_with_services(
+    hass: HomeAssistant, mock_api_fetchers: AsyncMock
+):
     """Set up the Marees France integration with a config entry for service testing."""
     entry = MockConfigEntry(
         domain=DOMAIN,
@@ -31,6 +33,7 @@
     await hass.async_block_till_done()
     return entry
 
+
 # Placeholder: To be expanded if the integration has custom services.
 # As of now, it's assumed there might not be specific custom services
 # beyond what Home Assistant or the coordinator handles internally.
@@ -92,11 +95,13 @@
 #             blocking=True,
 #         )
 
+
 # If there are no custom services, this file can remain minimal.
 # For example, a simple test to ensure the file is picked up by pytest:
 def test_placeholder_services():
     """Placeholder test to ensure the file is valid."""
     assert True
 
+
 # Add more tests here if the integration defines custom services.
-# Remember to mock dependencies and verify outcomes.
\ No newline at end of file
+# Remember to mock dependencies and verify outcomes.

--- tests/test_setup.py
+++ tests/test_setup.py
@@ -1,4 +1,5 @@
 """Test basic setup of the integration."""
+
 import pytest
 from unittest.mock import patch
 
@@ -8,11 +9,11 @@
 
 class MockJSModuleRegistration:
     """Mock JSModuleRegistration class."""
-    
+
     def __init__(self, *args, **kwargs):
         """Initialize."""
         pass
-        
+
     async def async_register(self):
         """Mock register method."""
         return True
@@ -21,18 +22,26 @@
 @pytest.fixture(autouse=True)
 def mock_frontend():
     """Mock the frontend module."""
-    with patch("custom_components.marees_france.frontend.JSModuleRegistration", MockJSModuleRegistration):
+    with patch(
+        "custom_components.marees_france.frontend.JSModuleRegistration",
+        MockJSModuleRegistration,
+    ):
         yield
 
 
 async def test_setup_integration(hass):
     """Test the integration can be set up."""
     # Mock the frontend module
-    with patch("custom_components.marees_france.__init__.JSModuleRegistration", MockJSModuleRegistration), \
-         patch("custom_components.marees_france.async_setup", return_value=True):
+    with (
+        patch(
+            "custom_components.marees_france.__init__.JSModuleRegistration",
+            MockJSModuleRegistration,
+        ),
+        patch("custom_components.marees_france.async_setup", return_value=True),
+    ):
         # Try to set up the component
         result = await async_setup_component(hass, DOMAIN, {})
         await hass.async_block_till_done()
-        
+
         # Check if the setup was successful
-        assert result is True, "Failed to set up the integration"
\ No newline at end of file
+        assert result is True, "Failed to set up the integration"

--- tests/test_setup_with_fixtures.py
+++ tests/test_setup_with_fixtures.py
@@ -1,4 +1,5 @@
 """Test setup with explicit fixtures."""
+
 from unittest.mock import patch, MagicMock
 
 import pytest
@@ -13,6 +14,7 @@
     """Enable custom integrations defined in the test dir."""
     yield
 
+
 @pytest.fixture(autouse=True)
 def expected_lingering_timers():
     """Mark that we expect lingering timers in this test module."""
@@ -25,8 +27,11 @@
     """Mock the frontend module."""
     mock_js_module = MagicMock()
     mock_js_module.async_register = MagicMock(return_value=True)
-    
-    with patch("custom_components.marees_france.frontend.JSModuleRegistration", return_value=mock_js_module):
+
+    with patch(
+        "custom_components.marees_france.frontend.JSModuleRegistration",
+        return_value=mock_js_module,
+    ):
         yield
 
 
@@ -34,16 +39,25 @@
     """Test setup with fixtures."""
     # Set up the component
     # Mock the JSModuleRegistration to prevent lingering timers
-    with patch("custom_components.marees_france.__init__.JSModuleRegistration") as mock_js, \
-         patch("custom_components.marees_france.frontend.JSModuleRegistration._async_wait_for_lovelace_resources", return_value=None), \
-         patch("homeassistant.helpers.event.async_call_later", return_value=lambda: None):
+    with (
+        patch(
+            "custom_components.marees_france.__init__.JSModuleRegistration"
+        ) as mock_js,
+        patch(
+            "custom_components.marees_france.frontend.JSModuleRegistration._async_wait_for_lovelace_resources",
+            return_value=None,
+        ),
+        patch(
+            "homeassistant.helpers.event.async_call_later", return_value=lambda: None
+        ),
+    ):
         mock_instance = MagicMock()
         mock_instance.async_register = MagicMock(return_value=True)
         mock_js.return_value = mock_instance
-        
+
         # Try to set up the component
         result = await async_setup_component(hass, DOMAIN, {})
         await hass.async_block_till_done()
-        
+
         # Check if the setup was successful
-        assert result is True
\ No newline at end of file
+        assert result is True

--- tools/sphinx-docs/conf.py
+++ tools/sphinx-docs/conf.py
@@ -5,31 +5,34 @@
 
 import os
 import sys
-sys.path.insert(0, os.path.abspath('../../custom_components')) # Adjust path to find marees_france
 
+sys.path.insert(
+    0, os.path.abspath("../../custom_components")
+)  # Adjust path to find marees_france
+
 # -- Project information -----------------------------------------------------
 # https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information
 
-project = 'Marées France Integration'
-copyright = '2025, The Marées France Authors'
-author = 'The Marées France Authors'
+project = "Marées France Integration"
+copyright = "2025, The Marées France Authors"
+author = "The Marées France Authors"
 
 # -- General configuration ---------------------------------------------------
 # https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration
 
 extensions = [
-    'sphinx.ext.autodoc',
-    'sphinx.ext.napoleon',  # For Google and NumPy style docstrings
-    'sphinx.ext.viewcode',  # To add links to source code
-    'sphinx.ext.intersphinx', # To link to other projects' documentation
+    "sphinx.ext.autodoc",
+    "sphinx.ext.napoleon",  # For Google and NumPy style docstrings
+    "sphinx.ext.viewcode",  # To add links to source code
+    "sphinx.ext.intersphinx",  # To link to other projects' documentation
 ]
 
-templates_path = ['_templates']
-exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']
+templates_path = ["_templates"]
+exclude_patterns = ["_build", "Thumbs.db", ".DS_Store"]
 
 # Napoleon settings (for Google style docstrings)
 napoleon_google_docstring = True
-napoleon_numpy_docstring = False # Or True if you use NumPy style
+napoleon_numpy_docstring = False  # Or True if you use NumPy style
 napoleon_include_init_with_doc = True
 napoleon_include_private_with_doc = False
 napoleon_include_special_with_doc = True
@@ -45,18 +48,18 @@
 
 # Intersphinx mapping
 intersphinx_mapping = {
-    'python': ('https://docs.python.org/3', None),
+    "python": ("https://docs.python.org/3", None),
     # 'homeassistant': ('https://developers.home-assistant.io/en/latest/', None), # Commented out as objects.inv may not be available
     # Add other mappings if needed, e.g., for aiohttp
     # 'aiohttp': ('https://docs.aiohttp.org/en/stable/', None),
 }
 
 # Autodoc settings
-autodoc_member_order = 'bysource' # Order members by source order
+autodoc_member_order = "bysource"  # Order members by source order
 autodoc_default_options = {
-    'members': True,
-    'undoc-members': True, # Include members without docstrings (though we aim for full coverage)
-    'show-inheritance': True,
+    "members": True,
+    "undoc-members": True,  # Include members without docstrings (though we aim for full coverage)
+    "show-inheritance": True,
 }
 # autodoc_typehints = "description" # Show typehints in the description (requires Sphinx 4.0+)
 # If using older Sphinx, you might need sphinx_autodoc_typehints extension
@@ -64,19 +67,19 @@
 # -- Options for HTML output -------------------------------------------------
 # https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output
 
-html_theme = 'sphinx_rtd_theme'
-html_static_path = ['_static']
+html_theme = "sphinx_rtd_theme"
+html_static_path = ["_static"]
 
 # If you have a logo:
 # html_logo = "_static/logo.png"
 
 # Theme options are theme-specific
 html_theme_options = {
-    'collapse_navigation': False,
-    'sticky_navigation': True,
-    'navigation_depth': 4,
-    'includehidden': True,
-    'titles_only': False
+    "collapse_navigation": False,
+    "sticky_navigation": True,
+    "navigation_depth": 4,
+    "includehidden": True,
+    "titles_only": False,
 }
 
 # Add any custom CSS files (optional)

